<!DOCTYPE html>
<!-- saved from url=(0080)https://www.cnblogs.com/LeftNotEasy/archive/2011/01/19/svd-and-applications.html -->
<html lang="zh-cn"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="referrer" content="origin">
    <meta property="og:description" content="版权声明： 本文由LeftNotEasy发布于http://leftnoteasy.cnblogs.com, 本文可以被全部的转载或者部分使用，但请注明出处，如果有问题，请联系wheeleast@gm">
    <meta http-equiv="Cache-Control" content="no-transform">
    <meta http-equiv="Cache-Control" content="no-siteapp">
    <title>机器学习中的数学(5)-强大的矩阵奇异值分解(SVD)及其应用 - LeftNotEasy - 博客园</title>
    
    <link rel="stylesheet" href="./机器学习中的数学(5)-强大的矩阵奇异值分解(SVD)及其应用 - LeftNotEasy - 博客园_files/blog-common.min.css">
    <link id="MainCss" rel="stylesheet" href="./机器学习中的数学(5)-强大的矩阵奇异值分解(SVD)及其应用 - LeftNotEasy - 博客园_files/bundle-simplememory.min.css">
    
    <link id="mobile-style" media="only screen and (max-width: 767px)" type="text/css" rel="stylesheet" href="./机器学习中的数学(5)-强大的矩阵奇异值分解(SVD)及其应用 - LeftNotEasy - 博客园_files/bundle-SimpleMemory-mobile.min.css">
    
    <link type="application/rss+xml" rel="alternate" href="https://www.cnblogs.com/LeftNotEasy/rss">
    <link type="application/rsd+xml" rel="EditURI" href="https://www.cnblogs.com/LeftNotEasy/rsd.xml">
    <link type="application/wlwmanifest+xml" rel="wlwmanifest" href="https://www.cnblogs.com/LeftNotEasy/wlwmanifest.xml">
    <script src="./机器学习中的数学(5)-强大的矩阵奇异值分解(SVD)及其应用 - LeftNotEasy - 博客园_files/amp4ads-host-v0.js"></script><script src="./机器学习中的数学(5)-强大的矩阵奇异值分解(SVD)及其应用 - LeftNotEasy - 博客园_files/osd.js"></script><script src="./机器学习中的数学(5)-强大的矩阵奇异值分解(SVD)及其应用 - LeftNotEasy - 博客园_files/pubads_impl_rendering_2019082901.js"></script><script async="" src="./机器学习中的数学(5)-强大的矩阵奇异值分解(SVD)及其应用 - LeftNotEasy - 博客园_files/analytics.js"></script><script src="./机器学习中的数学(5)-强大的矩阵奇异值分解(SVD)及其应用 - LeftNotEasy - 博客园_files/jquery-2.2.0.min.js"></script>
    <script src="./机器学习中的数学(5)-强大的矩阵奇异值分解(SVD)及其应用 - LeftNotEasy - 博客园_files/blog-common.min.js"></script>
    <script>
        var currentBlogId = 62514;
        var currentBlogApp = 'LeftNotEasy';
        var cb_enable_mathjax = true;
        var isLogined = false;
    </script>
    <script type="text/x-mathjax-config;executed=true">
        MathJax.Hub.Config({
        tex2jax: { inlineMath: [['$','$'], ['\\(','\\)']], processClass: 'math', processEscapes: true },
        TeX: {
        equationNumbers: { autoNumber: ['AMS'], useLabelIds: true },
        extensions: ['extpfeil.js', 'mediawiki-texvc.js'],
        Macros: {bm: "\\boldsymbol"}
        },
        'HTML-CSS': { linebreaks: { automatic: true } },
        SVG: { linebreaks: { automatic: true } }
        });
    </script>
    <script src="./机器学习中的数学(5)-强大的矩阵奇异值分解(SVD)及其应用 - LeftNotEasy - 博客园_files/MathJax.js"></script>
    
<link rel="preload" href="./机器学习中的数学(5)-强大的矩阵奇异值分解(SVD)及其应用 - LeftNotEasy - 博客园_files/f(4).txt" as="script"><script type="text/javascript" src="./机器学习中的数学(5)-强大的矩阵奇异值分解(SVD)及其应用 - LeftNotEasy - 博客园_files/f(4).txt"></script><link rel="preload" href="./机器学习中的数学(5)-强大的矩阵奇异值分解(SVD)及其应用 - LeftNotEasy - 博客园_files/f(5).txt" as="script"><script type="text/javascript" src="./机器学习中的数学(5)-强大的矩阵奇异值分解(SVD)及其应用 - LeftNotEasy - 博客园_files/f(5).txt"></script><script src="./机器学习中的数学(5)-强大的矩阵奇异值分解(SVD)及其应用 - LeftNotEasy - 博客园_files/pubads_impl_2019082901.js" async=""></script><style type="text/css">.MathJax_Hover_Frame {border-radius: .25em; -webkit-border-radius: .25em; -moz-border-radius: .25em; -khtml-border-radius: .25em; box-shadow: 0px 0px 15px #83A; -webkit-box-shadow: 0px 0px 15px #83A; -moz-box-shadow: 0px 0px 15px #83A; -khtml-box-shadow: 0px 0px 15px #83A; border: 1px solid #A6D ! important; display: inline-block; position: absolute}
.MathJax_Menu_Button .MathJax_Hover_Arrow {position: absolute; cursor: pointer; display: inline-block; border: 2px solid #AAA; border-radius: 4px; -webkit-border-radius: 4px; -moz-border-radius: 4px; -khtml-border-radius: 4px; font-family: 'Courier New',Courier; font-size: 9px; color: #F0F0F0}
.MathJax_Menu_Button .MathJax_Hover_Arrow span {display: block; background-color: #AAA; border: 1px solid; border-radius: 3px; line-height: 0; padding: 4px}
.MathJax_Hover_Arrow:hover {color: white!important; border: 2px solid #CCC!important}
.MathJax_Hover_Arrow:hover span {background-color: #CCC!important}
</style><style type="text/css">#MathJax_About {position: fixed; left: 50%; width: auto; text-align: center; border: 3px outset; padding: 1em 2em; background-color: #DDDDDD; color: black; cursor: default; font-family: message-box; font-size: 120%; font-style: normal; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; z-index: 201; border-radius: 15px; -webkit-border-radius: 15px; -moz-border-radius: 15px; -khtml-border-radius: 15px; box-shadow: 0px 10px 20px #808080; -webkit-box-shadow: 0px 10px 20px #808080; -moz-box-shadow: 0px 10px 20px #808080; -khtml-box-shadow: 0px 10px 20px #808080; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
#MathJax_About.MathJax_MousePost {outline: none}
.MathJax_Menu {position: absolute; background-color: white; color: black; width: auto; padding: 5px 0px; border: 1px solid #CCCCCC; margin: 0; cursor: default; font: menu; text-align: left; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; z-index: 201; border-radius: 5px; -webkit-border-radius: 5px; -moz-border-radius: 5px; -khtml-border-radius: 5px; box-shadow: 0px 10px 20px #808080; -webkit-box-shadow: 0px 10px 20px #808080; -moz-box-shadow: 0px 10px 20px #808080; -khtml-box-shadow: 0px 10px 20px #808080; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
.MathJax_MenuItem {padding: 1px 2em; background: transparent}
.MathJax_MenuArrow {position: absolute; right: .5em; padding-top: .25em; color: #666666; font-size: .75em}
.MathJax_MenuActive .MathJax_MenuArrow {color: white}
.MathJax_MenuArrow.RTL {left: .5em; right: auto}
.MathJax_MenuCheck {position: absolute; left: .7em}
.MathJax_MenuCheck.RTL {right: .7em; left: auto}
.MathJax_MenuRadioCheck {position: absolute; left: .7em}
.MathJax_MenuRadioCheck.RTL {right: .7em; left: auto}
.MathJax_MenuLabel {padding: 1px 2em 3px 1.33em; font-style: italic}
.MathJax_MenuRule {border-top: 1px solid #DDDDDD; margin: 4px 3px}
.MathJax_MenuDisabled {color: GrayText}
.MathJax_MenuActive {background-color: #606872; color: white}
.MathJax_MenuDisabled:focus, .MathJax_MenuLabel:focus {background-color: #E8E8E8}
.MathJax_ContextMenu:focus {outline: none}
.MathJax_ContextMenu .MathJax_MenuItem:focus {outline: none}
#MathJax_AboutClose {top: .2em; right: .2em}
.MathJax_Menu .MathJax_MenuClose {top: -10px; left: -10px}
.MathJax_MenuClose {position: absolute; cursor: pointer; display: inline-block; border: 2px solid #AAA; border-radius: 18px; -webkit-border-radius: 18px; -moz-border-radius: 18px; -khtml-border-radius: 18px; font-family: 'Courier New',Courier; font-size: 24px; color: #F0F0F0}
.MathJax_MenuClose span {display: block; background-color: #AAA; border: 1.5px solid; border-radius: 18px; -webkit-border-radius: 18px; -moz-border-radius: 18px; -khtml-border-radius: 18px; line-height: 0; padding: 8px 0 6px}
.MathJax_MenuClose:hover {color: white!important; border: 2px solid #CCC!important}
.MathJax_MenuClose:hover span {background-color: #CCC!important}
.MathJax_MenuClose:hover:focus {outline: none}
</style><style type="text/css">.MathJax_Preview .MJXf-math {color: inherit!important}
</style><style type="text/css">.MJX_Assistive_MathML {position: absolute!important; top: 0; left: 0; clip: rect(1px, 1px, 1px, 1px); padding: 1px 0 0 0!important; border: 0!important; height: 1px!important; width: 1px!important; overflow: hidden!important; display: block!important; -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none}
.MJX_Assistive_MathML.MJX_Assistive_MathML_Block {width: 100%!important}
</style><style type="text/css">#MathJax_Zoom {position: absolute; background-color: #F0F0F0; overflow: auto; display: block; z-index: 301; padding: .5em; border: 1px solid black; margin: 0; font-weight: normal; font-style: normal; text-align: left; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; -webkit-box-sizing: content-box; -moz-box-sizing: content-box; box-sizing: content-box; box-shadow: 5px 5px 15px #AAAAAA; -webkit-box-shadow: 5px 5px 15px #AAAAAA; -moz-box-shadow: 5px 5px 15px #AAAAAA; -khtml-box-shadow: 5px 5px 15px #AAAAAA; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
#MathJax_ZoomOverlay {position: absolute; left: 0; top: 0; z-index: 300; display: inline-block; width: 100%; height: 100%; border: 0; padding: 0; margin: 0; background-color: white; opacity: 0; filter: alpha(opacity=0)}
#MathJax_ZoomFrame {position: relative; display: inline-block; height: 0; width: 0}
#MathJax_ZoomEventTrap {position: absolute; left: 0; top: 0; z-index: 302; display: inline-block; border: 0; padding: 0; margin: 0; background-color: white; opacity: 0; filter: alpha(opacity=0)}
</style><style type="text/css">.MathJax_Preview {color: #888}
#MathJax_Message {position: fixed; left: 1em; bottom: 1.5em; background-color: #E6E6E6; border: 1px solid #959595; margin: 0px; padding: 2px 8px; z-index: 102; color: black; font-size: 80%; width: auto; white-space: nowrap}
#MathJax_MSIE_Frame {position: absolute; top: 0; left: 0; width: 0px; z-index: 101; border: 0px; margin: 0px; padding: 0px}
.MathJax_Error {color: #CC0000; font-style: italic}
</style><style type="text/css">.MJXp-script {font-size: .8em}
.MJXp-right {-webkit-transform-origin: right; -moz-transform-origin: right; -ms-transform-origin: right; -o-transform-origin: right; transform-origin: right}
.MJXp-bold {font-weight: bold}
.MJXp-italic {font-style: italic}
.MJXp-scr {font-family: MathJax_Script,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-frak {font-family: MathJax_Fraktur,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-sf {font-family: MathJax_SansSerif,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-cal {font-family: MathJax_Caligraphic,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-mono {font-family: MathJax_Typewriter,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-largeop {font-size: 150%}
.MJXp-largeop.MJXp-int {vertical-align: -.2em}
.MJXp-math {display: inline-block; line-height: 1.2; text-indent: 0; font-family: 'Times New Roman',Times,STIXGeneral,serif; white-space: nowrap; border-collapse: collapse}
.MJXp-display {display: block; text-align: center; margin: 1em 0}
.MJXp-math span {display: inline-block}
.MJXp-box {display: block!important; text-align: center}
.MJXp-box:after {content: " "}
.MJXp-rule {display: block!important; margin-top: .1em}
.MJXp-char {display: block!important}
.MJXp-mo {margin: 0 .15em}
.MJXp-mfrac {margin: 0 .125em; vertical-align: .25em}
.MJXp-denom {display: inline-table!important; width: 100%}
.MJXp-denom > * {display: table-row!important}
.MJXp-surd {vertical-align: top}
.MJXp-surd > * {display: block!important}
.MJXp-script-box > *  {display: table!important; height: 50%}
.MJXp-script-box > * > * {display: table-cell!important; vertical-align: top}
.MJXp-script-box > *:last-child > * {vertical-align: bottom}
.MJXp-script-box > * > * > * {display: block!important}
.MJXp-mphantom {visibility: hidden}
.MJXp-munderover, .MJXp-munder {display: inline-table!important}
.MJXp-over {display: inline-block!important; text-align: center}
.MJXp-over > * {display: block!important}
.MJXp-munderover > *, .MJXp-munder > * {display: table-row!important}
.MJXp-mtable {vertical-align: .25em; margin: 0 .125em}
.MJXp-mtable > * {display: inline-table!important; vertical-align: middle}
.MJXp-mtr {display: table-row!important}
.MJXp-mtd {display: table-cell!important; text-align: center; padding: .5em 0 0 .5em}
.MJXp-mtr > .MJXp-mtd:first-child {padding-left: 0}
.MJXp-mtr:first-child > .MJXp-mtd {padding-top: 0}
.MJXp-mlabeledtr {display: table-row!important}
.MJXp-mlabeledtr > .MJXp-mtd:first-child {padding-left: 0}
.MJXp-mlabeledtr:first-child > .MJXp-mtd {padding-top: 0}
.MJXp-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 1px 3px; font-style: normal; font-size: 90%}
.MJXp-scale0 {-webkit-transform: scaleX(.0); -moz-transform: scaleX(.0); -ms-transform: scaleX(.0); -o-transform: scaleX(.0); transform: scaleX(.0)}
.MJXp-scale1 {-webkit-transform: scaleX(.1); -moz-transform: scaleX(.1); -ms-transform: scaleX(.1); -o-transform: scaleX(.1); transform: scaleX(.1)}
.MJXp-scale2 {-webkit-transform: scaleX(.2); -moz-transform: scaleX(.2); -ms-transform: scaleX(.2); -o-transform: scaleX(.2); transform: scaleX(.2)}
.MJXp-scale3 {-webkit-transform: scaleX(.3); -moz-transform: scaleX(.3); -ms-transform: scaleX(.3); -o-transform: scaleX(.3); transform: scaleX(.3)}
.MJXp-scale4 {-webkit-transform: scaleX(.4); -moz-transform: scaleX(.4); -ms-transform: scaleX(.4); -o-transform: scaleX(.4); transform: scaleX(.4)}
.MJXp-scale5 {-webkit-transform: scaleX(.5); -moz-transform: scaleX(.5); -ms-transform: scaleX(.5); -o-transform: scaleX(.5); transform: scaleX(.5)}
.MJXp-scale6 {-webkit-transform: scaleX(.6); -moz-transform: scaleX(.6); -ms-transform: scaleX(.6); -o-transform: scaleX(.6); transform: scaleX(.6)}
.MJXp-scale7 {-webkit-transform: scaleX(.7); -moz-transform: scaleX(.7); -ms-transform: scaleX(.7); -o-transform: scaleX(.7); transform: scaleX(.7)}
.MJXp-scale8 {-webkit-transform: scaleX(.8); -moz-transform: scaleX(.8); -ms-transform: scaleX(.8); -o-transform: scaleX(.8); transform: scaleX(.8)}
.MJXp-scale9 {-webkit-transform: scaleX(.9); -moz-transform: scaleX(.9); -ms-transform: scaleX(.9); -o-transform: scaleX(.9); transform: scaleX(.9)}
.MathJax_PHTML .noError {vertical-align: ; font-size: 90%; text-align: left; color: black; padding: 1px 3px; border: 1px solid}
</style><link rel="prefetch" href="https://tpc.googlesyndication.com/safeframe/1-0-35/html/container.html"></head>
<body><div id="MathJax_Message" style="display: none;"></div>
    <a name="top"></a>
    
    
<!--done-->
<div id="home">
<div id="header">
	<div id="blogTitle">
        <a id="lnkBlogLogo" href="https://www.cnblogs.com/LeftNotEasy/"><img id="blogLogo" src="./机器学习中的数学(5)-强大的矩阵奇异值分解(SVD)及其应用 - LeftNotEasy - 博客园_files/logo.gif" alt="返回主页"></a>		
		
<!--done-->
<h1><a id="Header1_HeaderTitle" class="headermaintitle HeaderMainTitle" href="https://www.cnblogs.com/LeftNotEasy/">LeftNotEasy - Wangda Tan</a>
</h1>
<h2>
关注于 机器学习、数据挖掘、并行计算、数学
</h2>




		
	</div><!--end: blogTitle 博客的标题和副标题 -->
	<div id="navigator">
		
<ul id="navList">
<li><a id="blog_nav_sitehome" class="menu" href="https://www.cnblogs.com/">
博客园</a>
</li>
<li>
<a id="blog_nav_myhome" class="menu" href="https://www.cnblogs.com/LeftNotEasy/">
首页</a>
</li>
<li>


</li>
<li>
<a id="blog_nav_contact" class="menu" href="https://msg.cnblogs.com/send/LeftNotEasy">
联系</a></li>
<li>
<a id="blog_nav_rss" class="menu" href="https://www.cnblogs.com/LeftNotEasy/rss/">
订阅</a>
<!--<partial name="./Shared/_XmlLink.cshtml" model="Model" /></li>--></li>
<li>
<a id="blog_nav_admin" class="menu" href="https://i.cnblogs.com/">
管理</a>
</li>
</ul>


		<div class="blogStats">
			
			<span id="stats_post_count">随笔 - 
45&nbsp; </span>
<span id="stats_article_count">文章 - 
0&nbsp; </span>
<span id="stats-comment_count">评论 - 
462</span>

			
		</div><!--end: blogStats -->
	</div><!--end: navigator 博客导航栏 -->
</div><!--end: header 头部 -->

<div id="main">
	<div id="mainContent">
	<div class="forFlow">
		<div id="post_detail">
    <!--done-->
    <div id="topics">
        <div class="post">
            <h1 class="postTitle">
                
<a id="cb_post_title_url" class="postTitle2" href="https://www.cnblogs.com/LeftNotEasy/archive/2011/01/19/svd-and-applications.html">机器学习中的数学(5)-强大的矩阵奇异值分解(SVD)及其应用</a>

            </h1>
            <div class="clear"></div>
            <div class="postBody">
                
<div id="cnblogs_post_body" class="blogpost-body ">
    <p><strong><span style="color: #0080ff; font-size: large;">版权声明：</span></strong></p>
<p>&nbsp;&nbsp;&nbsp; 本文由LeftNotEasy发布于<a href="http://leftnoteasy.cnblogs.com/">http://leftnoteasy.cnblogs.com</a>, 本文可以被全部的转载或者部分使用，但请注明出处，如果有问题，请联系<a href="mailto:wheeleast@gmail.com">wheeleast@gmail.com</a>。也可以加我的微博: <a href="http://weibo.com/leftnoteasy" target="_blank">@leftnoteasy</a></p>
<p><strong><span style="color: #0080ff; font-size: large;">前言：</span></strong></p>
<p>&nbsp;&nbsp;&nbsp; 上一次写了关于<a href="http://www.cnblogs.com/LeftNotEasy/archive/2011/01/08/lda-and-pca-machine-learning.html">PCA与LDA</a>的文章，PCA的实现一般有两种，一种是用特征值分解去实现的，一种是用奇异值分解去实现的。在上篇文章中便是基于特征值分解的一种解释。特征值和奇异值在大部分人的印象中，往往是停留在纯粹的数学计算中。而且线性代数或者矩阵论里面，也很少讲任何跟特征值与奇异值有关的应用背景。奇异值分解是一个有着很明显的物理意义的一种方法，它可以将一个比较复杂的矩阵用更小更简单的几个子矩阵的相乘来表示，这些小矩阵描述的是矩阵的重要的特性。就像是描述一个人一样，给别人描述说这个人长得浓眉大眼，方脸，络腮胡，而且带个黑框的眼镜，这样寥寥的几个特征，就让别人脑海里面就有一个较为清楚的认识，实际上，人脸上的特征是有着无数种的，之所以能这么描述，是因为人天生就有着非常好的抽取重要特征的能力，让机器学会抽取重要的特征，SVD是一个重要的方法。</p>
<p>&nbsp;&nbsp;&nbsp; 在机器学习领域，有相当多的应用与奇异值都可以扯上关系，比如做feature reduction的PCA，做数据压缩（以图像压缩为代表）的算法，还有做搜索引擎语义层次检索的LSI（Latent Semantic Indexing）</p>
<p>&nbsp;&nbsp;&nbsp; 另外在这里抱怨一下，之前在百度里面搜索过SVD，出来的结果都是俄罗斯的一种狙击枪（AK47同时代的），是因为穿越火线这个游戏里面有一把狙击枪叫做SVD，而在Google上面搜索的时候，出来的都是奇异值分解（英文资料为主）。想玩玩战争游戏，玩玩COD不是非常好吗，玩山寨的CS有神马意思啊。国内的网页中的话语权也被这些没有太多营养的帖子所占据。真心希望国内的气氛能够更浓一点，搞游戏的人真正是喜欢制作游戏，搞Data Mining的人是真正喜欢挖数据的，都不是仅仅为了混口饭吃，这样谈超越别人才有意义，中文文章中，能踏踏实实谈谈技术的太少了，改变这个状况，从我自己做起吧。</p>
<p>&nbsp;&nbsp;&nbsp; 前面说了这么多，本文主要关注奇异值的一些特性，另外还会稍稍提及奇异值的计算，不过本文不准备在如何计算奇异值上展开太多。另外，本文里面有部分不算太深的线性代数的知识，如果完全忘记了线性代数，看本文可能会有些困难。</p>
<p><strong><span style="color: #0080ff; font-size: large;">一、奇异值与特征值基础知识：</span></strong></p>
<p>&nbsp;&nbsp;&nbsp; 特征值分解和奇异值分解在机器学习领域都是属于满地可见的方法。两者有着很紧密的关系，我在接下来会谈到，特征值分解和奇异值分解的目的都是一样，就是提取出一个矩阵最重要的特征。先谈谈特征值分解吧：</p>
<p><span style="font-size: large;">&nbsp;&nbsp; 1）<strong>特征值：</strong></span></p>
<p>&nbsp;&nbsp;&nbsp; 如果说一个向量v是方阵A的特征向量，将一定可以表示成下面的形式：</p>
<p><a href="http://images.cnblogs.com/cnblogs_com/LeftNotEasy/201101/201101192226321862.png"><img style="display: block; float: none; margin-left: auto; margin-right: auto; border-width: 0px;" title="image" src="./机器学习中的数学(5)-强大的矩阵奇异值分解(SVD)及其应用 - LeftNotEasy - 博客园_files/20110119222632467.png" alt="image" width="84" height="34" border="0"></a></p>
<p>&nbsp;&nbsp;&nbsp; 这时候λ就被称为特征向量v对应的特征值，一个矩阵的一组特征向量是一组正交向量。特征值分解是将一个矩阵分解成下面的形式：</p>
<p><a href="http://images.cnblogs.com/cnblogs_com/LeftNotEasy/201101/201101192226321023.png"><img style="display: block; float: none; margin-left: auto; margin-right: auto; border-width: 0px;" title="image" src="./机器学习中的数学(5)-强大的矩阵奇异值分解(SVD)及其应用 - LeftNotEasy - 博客园_files/201101192226327992.png" alt="image" width="112" height="38" border="0"></a></p>
<p>&nbsp;&nbsp;&nbsp; 其中Q是这个矩阵A的特征向量组成的矩阵，Σ是一个对角阵，每一个对角线上的元素就是一个特征值。我这里引用了一些参考文献中的内容来说明一下。首先，要明确的是，一个矩阵其实就是一个线性变换，因为一个矩阵乘以一个向量后得到的向量，其实就相当于将这个向量进行了线性变换。比如说下面的一个矩阵：</p>
<p>&nbsp;&nbsp;&nbsp; <a href="http://images.cnblogs.com/cnblogs_com/LeftNotEasy/201101/20110119222632500.png"><img style="display: block; float: none; margin-left: auto; margin-right: auto; border-width: 0px;" title="image" src="./机器学习中的数学(5)-强大的矩阵奇异值分解(SVD)及其应用 - LeftNotEasy - 博客园_files/201101192226323008.png" alt="image" width="105" height="55" border="0"></a>&nbsp;&nbsp;&nbsp; 它其实对应的线性变换是下面的形式：</p>
<p><a href="http://images.cnblogs.com/cnblogs_com/LeftNotEasy/201101/201101192226326073.png"><img style="display: block; float: none; margin-left: auto; margin-right: auto; border-width: 0px;" title="image" src="./机器学习中的数学(5)-强大的矩阵奇异值分解(SVD)及其应用 - LeftNotEasy - 博客园_files/201101192226323913.png" alt="image" width="371" height="170" border="0"></a>&nbsp;&nbsp;&nbsp; 因为这个矩阵M乘以一个向量(x,y)的结果是：</p>
<p><a href="http://images.cnblogs.com/cnblogs_com/LeftNotEasy/201101/201101192226334470.png"><img style="display: block; float: none; margin-left: auto; margin-right: auto; border-width: 0px;" title="image" src="./机器学习中的数学(5)-强大的矩阵奇异值分解(SVD)及其应用 - LeftNotEasy - 博客园_files/201101192226335026.png" alt="image" width="162" height="58" border="0"></a>&nbsp;&nbsp;&nbsp; 上面的矩阵是对称的，所以这个变换是一个对x，y轴的方向一个拉伸变换（每一个对角线上的元素将会对一个维度进行拉伸变换，当值&gt;1时，是拉长，当值&lt;1时时缩短），当矩阵不是对称的时候，假如说矩阵是下面的样子：</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p><a href="http://images.cnblogs.com/cnblogs_com/LeftNotEasy/201101/201101192226337534.png"><img style="display: block; float: none; margin-left: auto; margin-right: auto; border-width: 0px;" title="image" src="./机器学习中的数学(5)-强大的矩阵奇异值分解(SVD)及其应用 - LeftNotEasy - 博客园_files/201101192226336454.png" alt="image" width="100" height="53" border="0"></a></p>
<p>&nbsp;&nbsp;&nbsp; 它所描述的变换是下面的样子：</p>
<p><a href="http://images.cnblogs.com/cnblogs_com/LeftNotEasy/201101/201101192226333107.png"><img style="display: block; float: none; margin-left: auto; margin-right: auto; border-width: 0px;" title="image" src="./机器学习中的数学(5)-强大的矩阵奇异值分解(SVD)及其应用 - LeftNotEasy - 博客园_files/201101192226334536.png" alt="image" width="366" height="171" border="0"></a></p>
<p>&nbsp;&nbsp;&nbsp; 这其实是在平面上对一个轴进行的拉伸变换（如蓝色的箭头所示），在图中，蓝色的箭头是一个最<strong>主要的</strong>变化方向（变化方向可能有不止一个），<strong>如果我们想要描述好一个变换，那我们就描述好这个变换主要的变化方向就好了</strong>。反过头来看看之前特征值分解的式子，分解得到的Σ矩阵是一个对角阵，里面的特征值是由大到小排列的，这些特征值所对应的特征向量就是描述这个矩阵变化方向（从主要的变化到次要的变化排列）</p>
<p>&nbsp;&nbsp;&nbsp; 当矩阵是高维的情况下，那么这个矩阵就是高维空间下的一个线性变换，这个线性变化可能没法通过图片来表示，但是可以想象，这个变换也同样有很多的变换方向，我们通过特征值分解得到的前N个特征向量，那么就对应了这个矩阵最主要的N个变化方向。我们利用这前N个变化方向，就可以近似这个矩阵（变换）。也就是之前说的：<strong>提取这个矩阵最重要的特征。</strong>总结一下，特征值分解可以得到特征值与特征向量，特征值表示的是这个特征到底有多重要，而特征向量表示这个特征是什么，可以将每一个特征向量理解为一个线性的子空间，我们可以利用这些线性的子空间干很多的事情。不过，<strong>特征值分解也有很多的局限，比如说变换的矩阵必须是方阵。</strong></p>
<p>&nbsp;&nbsp; （说了这么多特征值变换，不知道有没有说清楚，请各位多提提意见。）</p>
<p>&nbsp;</p>
<p><span style="font-size: large;"><strong>&nbsp;&nbsp; 2）奇异值：</strong></span></p>
<p>&nbsp;&nbsp;&nbsp; 下面谈谈奇异值分解。特征值分解是一个提取矩阵特征很不错的方法，但是它只是对方阵而言的，在现实的世界中，我们看到的大部分矩阵都不是方阵，比如说有N个学生，每个学生有M科成绩，这样形成的一个N * M的矩阵就不可能是方阵，<strong>我们怎样才能描述这样普通的矩阵呢的重要特征呢？</strong>奇异值分解可以用来干这个事情，<strong>奇异值分解是一个能适用于任意的矩阵的一种分解的方法</strong>：</p>
<p><a href="http://images.cnblogs.com/cnblogs_com/LeftNotEasy/201101/201101192226335092.png"><img style="display: block; float: none; margin-left: auto; margin-right: auto; border-width: 0px;" title="image" src="./机器学习中的数学(5)-强大的矩阵奇异值分解(SVD)及其应用 - LeftNotEasy - 博客园_files/201101192226332060.png" alt="image" width="118" height="40" border="0"></a>&nbsp;&nbsp;&nbsp; 假设A是一个N * M的矩阵，那么得到的U是一个N * N的方阵（里面的向量是正交的，U里面的向量称为左奇异向量），Σ是一个N * M的矩阵（除了对角线的元素都是0，对角线上的元素称为奇异值），V’(V的转置)是一个N * N的矩阵，里面的向量也是正交的，V里面的向量称为右奇异向量），从图片来反映几个相乘的矩阵的大小可得下面的图片</p>
<p><a href="http://images.cnblogs.com/cnblogs_com/LeftNotEasy/201101/201101192226341537.png"><img style="display: block; float: none; margin-left: auto; margin-right: auto; border-width: 0px;" title="image" src="./机器学习中的数学(5)-强大的矩阵奇异值分解(SVD)及其应用 - LeftNotEasy - 博客园_files/201101192226342650.png" alt="image" width="439" height="152" border="0"></a></p>
<p>&nbsp;&nbsp;&nbsp; 那么奇异值和特征值是怎么对应起来的呢？首先，我们将一个矩阵A的转置 * A，将会得到一个方阵，我们用这个方阵求特征值可以得到：<a href="http://images.cnblogs.com/cnblogs_com/LeftNotEasy/201101/201101192226349618.png"><img style="display: block; float: none; margin-left: auto; margin-right: auto; border-width: 0px;" title="image" src="./机器学习中的数学(5)-强大的矩阵奇异值分解(SVD)及其应用 - LeftNotEasy - 博客园_files/201101192226348223.png" alt="image" width="153" height="44" border="0"></a>&nbsp;&nbsp;&nbsp; 这里得到的v，就是我们上面的右奇异向量。此外我们还可以得到：</p>
<p><a href="http://images.cnblogs.com/cnblogs_com/LeftNotEasy/201101/201101192226344111.png"><img style="display: block; float: none; margin-left: auto; margin-right: auto; border-width: 0px;" title="image" src="./机器学习中的数学(5)-强大的矩阵奇异值分解(SVD)及其应用 - LeftNotEasy - 博客园_files/201101192226346304.png" alt="image" width="100" height="104" border="0"></a>&nbsp;&nbsp;&nbsp; 这里的σ就是上面说的奇异值，u就是上面说的左奇异向量。奇异值σ跟特征值类似，在矩阵Σ中也是从大到小排列，而且σ的减少特别的快，<strong>在很多情况下，前10%甚至1%的奇异值的和就占了全部的奇异值之和的99%以上了</strong>。也就是说，我们也可以用前r大的奇异值来近似描述矩阵，这里定义一下<strong>部分奇异值分解</strong>：</p>
<p><a href="http://images.cnblogs.com/cnblogs_com/LeftNotEasy/201101/201101192226359684.png"><img style="display: block; float: none; margin-left: auto; margin-right: auto; border-width: 0px;" title="image" src="./机器学习中的数学(5)-强大的矩阵奇异值分解(SVD)及其应用 - LeftNotEasy - 博客园_files/201101192226358289.png" alt="image" width="204" height="45" border="0"></a></p>
<p>&nbsp;&nbsp;&nbsp; r是一个远小于m、n的数，这样矩阵的乘法看起来像是下面的样子：</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p><a href="http://images.cnblogs.com/cnblogs_com/LeftNotEasy/201101/201101192226359717.png"><img style="display: block; float: none; margin-left: auto; margin-right: auto; border-width: 0px;" title="image" src="./机器学习中的数学(5)-强大的矩阵奇异值分解(SVD)及其应用 - LeftNotEasy - 博客园_files/201101192226356370.png" alt="image" width="352" height="184" border="0"></a></p>
<p>&nbsp;&nbsp;&nbsp; 右边的三个矩阵相乘的结果将会是一个接近于A的矩阵，在这儿，r越接近于n，则相乘的结果越接近于A。而这三个矩阵的面积之和（在存储观点来说，矩阵面积越小，存储量就越小）要远远小于原始的矩阵A，我们如果想要压缩空间来表示原矩阵A，我们存下这里的三个矩阵：U、Σ、V就好了。</p>
<p>&nbsp;</p>
<p><span style="color: #0080ff; font-size: large;"><strong>二、奇异值的计算：</strong></span></p>
<p>&nbsp;&nbsp;&nbsp; 奇异值的计算是一个难题，是一个O(N^3)的算法。在单机的情况下当然是没问题的，matlab在一秒钟内就可以算出1000 * 1000的矩阵的所有奇异值，但是当矩阵的规模增长的时候，计算的复杂度呈3次方增长，就需要并行计算参与了。Google的<strong>吴军</strong>老师在<strong>数学之美</strong>系列谈到SVD的时候，说起Google实现了SVD的并行化算法，说这是对人类的一个贡献，但是也没有给出具体的计算规模，也没有给出太多有价值的信息。</p>
<p>&nbsp;&nbsp;&nbsp; 其实SVD还是可以用并行的方式去实现的，在解大规模的矩阵的时候，一般使用迭代的方法，当矩阵的规模很大（比如说上亿）的时候，迭代的次数也可能会上亿次，如果使用Map-Reduce框架去解，则每次Map-Reduce完成的时候，都会涉及到写文件、读文件的操作。个人猜测Google云计算体系中除了Map-Reduce以外应该还有类似于MPI的计算模型，也就是节点之间是保持通信，数据是常驻在内存中的，这种计算模型比Map-Reduce在解决迭代次数非常多的时候，要快了很多倍。</p>
<p>&nbsp;&nbsp;&nbsp; <a href="http://en.wikipedia.org/wiki/Lanczos_algorithm">Lanczos迭代</a>就是一种解<strong>对称方阵部分特征值</strong>的方法（之前谈到了，解A’* A得到的对称方阵的特征值就是解A的右奇异向量），是将一个对称的方程化为一个三对角矩阵再进行求解。按网上的一些文献来看，Google应该是用这种方法去做的奇异值分解的。请见Wikipedia上面的一些引用的论文，如果理解了那些论文，也“几乎”可以做出一个SVD了。</p>
<p>&nbsp;&nbsp;&nbsp; 由于奇异值的计算是一个很枯燥，纯数学的过程，而且前人的研究成果（论文中）几乎已经把整个程序的流程图给出来了。更多的关于奇异值计算的部分，将在后面的参考文献中给出，这里不再深入，我还是focus在奇异值的应用中去。</p>
<p>&nbsp;</p>
<p><span style="color: #0080ff; font-size: large;"><strong>三、奇异值与主成分分析（PCA）：</strong></span></p>
<p>&nbsp;&nbsp;&nbsp;&nbsp; 主成分分析在上一节里面也讲了一些，这里主要谈谈如何用SVD去解PCA的问题。PCA的问题其实是一个基的变换，使得变换后的数据有着最大的方差。方差的大小描述的是一个变量的信息量，我们在讲一个东西的稳定性的时候，往往说要减小方差，如果一个模型的方差很大，那就说明模型不稳定了。但是对于我们用于机器学习的数据（主要是训练数据），方差大才有意义，不然输入的数据都是同一个点，那方差就为0了，这样输入的多个数据就等同于一个数据了。以下面这张图为例子：</p>
<p><a href="http://images.cnblogs.com/cnblogs_com/LeftNotEasy/201101/201101192226359750.png"><img style="display: block; float: none; margin-left: auto; margin-right: auto; border-width: 0px;" title="image" src="./机器学习中的数学(5)-强大的矩阵奇异值分解(SVD)及其应用 - LeftNotEasy - 博客园_files/201101192226357275.png" alt="image" width="240" height="184" border="0"></a>&nbsp;&nbsp;&nbsp;&nbsp; 这个假设是一个摄像机采集一个物体运动得到的图片，上面的点表示物体运动的位置，假如我们想要用一条直线去拟合这些点，那我们会选择什么方向的线呢？当然是图上标有signal的那条线。如果我们把这些点单纯的投影到x轴或者y轴上，最后在x轴与y轴上得到的方差是相似的（因为这些点的趋势是在45度左右的方向，所以投影到x轴或者y轴上都是类似的），如果我们使用原来的xy坐标系去看这些点，容易看不出来这些点真正的方向是什么。但是如果我们进行坐标系的变化，横轴变成了signal的方向，纵轴变成了noise的方向，则就很容易发现什么方向的方差大，什么方向的方差小了。</p>
<p>&nbsp;&nbsp;&nbsp; 一般来说，方差大的方向是信号的方向，方差小的方向是噪声的方向，我们在数据挖掘中或者数字信号处理中，往往要提高信号与噪声的比例，也就是信噪比。对上图来说，如果我们只保留signal方向的数据，也可以对原数据进行不错的近似了。</p>
<p>&nbsp;&nbsp;&nbsp; PCA的全部工作简单点说，就是对原始的空间中顺序地找一组相互正交的坐标轴，第一个轴是使得方差最大的，第二个轴是在与第一个轴正交的平面中使得方差最大的，第三个轴是在与第1、2个轴正交的平面中方差最大的，这样假设在N维空间中，我们可以找到N个这样的坐标轴，我们取前r个去近似这个空间，这样就从一个N维的空间压缩到r维的空间了，但是我们选择的r个坐标轴能够使得空间的压缩使得数据的损失最小。</p>
<p>&nbsp;&nbsp;&nbsp; 还是假设我们矩阵每一行表示一个样本，每一列表示一个feature，用矩阵的语言来表示，将一个m * n的矩阵A的进行坐标轴的变化，P就是一个变换的矩阵从一个N维的空间变换到另一个N维的空间，在空间中就会进行一些类似于旋转、拉伸的变化。</p>
<p><a href="http://images.cnblogs.com/cnblogs_com/LeftNotEasy/201101/201101192226367831.png"><img style="display: block; float: none; margin-left: auto; margin-right: auto; border-width: 0px;" title="image" src="./机器学习中的数学(5)-强大的矩阵奇异值分解(SVD)及其应用 - LeftNotEasy - 博客园_files/201101192226366436.png" alt="image" width="160" height="44" border="0"></a></p>
<p>&nbsp;&nbsp;&nbsp; 而将一个m * n的矩阵A变换成一个m * r的矩阵，这样就会使得本来有n个feature的，变成了有r个feature了（r &lt; n)，这r个其实就是对n个feature的一种提炼，我们就把这个称为feature的压缩。用数学语言表示就是：</p>
<p><a href="http://images.cnblogs.com/cnblogs_com/LeftNotEasy/201101/201101192226361452.png"><img style="display: block; float: none; margin-left: auto; margin-right: auto; border-width: 0px;" title="image" src="./机器学习中的数学(5)-强大的矩阵奇异值分解(SVD)及其应用 - LeftNotEasy - 博客园_files/201101192226364833.png" alt="image" width="167" height="43" border="0"></a>&nbsp;&nbsp;&nbsp; 但是这个怎么和SVD扯上关系呢？之前谈到，SVD得出的奇异向量也是从奇异值由大到小排列的，按PCA的观点来看，就是方差最大的坐标轴就是第一个奇异向量，方差次大的坐标轴就是第二个奇异向量…我们回忆一下之前得到的SVD式子：</p>
<p><a href="http://images.cnblogs.com/cnblogs_com/LeftNotEasy/201101/201101192226363437.png"><img style="display: block; float: none; margin-left: auto; margin-right: auto; border-width: 0px;" title="image" src="./机器学习中的数学(5)-强大的矩阵奇异值分解(SVD)及其应用 - LeftNotEasy - 博客园_files/201101192226366818.png" alt="image" width="224" height="43" border="0"></a>&nbsp;&nbsp;&nbsp;&nbsp; 在矩阵的两边同时乘上一个矩阵V，由于V是一个正交的矩阵，所以V转置乘以V得到单位阵I，所以可以化成后面的式子</p>
<p><a href="http://images.cnblogs.com/cnblogs_com/LeftNotEasy/201101/201101192226365422.png"><img style="display: block; float: none; margin-left: auto; margin-right: auto; border-width: 0px;" title="image" src="./机器学习中的数学(5)-强大的矩阵奇异值分解(SVD)及其应用 - LeftNotEasy - 博客园_files/201101192226374899.png" alt="image" width="304" height="81" border="0"></a>&nbsp;&nbsp;&nbsp;&nbsp; 将后面的式子与A * P那个m * n的矩阵变换为m * r的矩阵的式子对照看看，在这里，其实V就是P，也就是一个变化的向量。这里是将一个m * n 的矩阵压缩到一个m * r的矩阵，也就是对列进行压缩，如果我们想对行进行压缩（在PCA的观点下，对行进行压缩可以理解为，将一些相似的sample合并在一起，或者将一些没有太大价值的sample去掉）怎么办呢？同样我们写出一个通用的行压缩例子：</p>
<p><a href="http://images.cnblogs.com/cnblogs_com/LeftNotEasy/201101/201101192226375455.png"><img style="display: block; float: none; margin-left: auto; margin-right: auto; border-width: 0px;" title="image" src="./机器学习中的数学(5)-强大的矩阵奇异值分解(SVD)及其应用 - LeftNotEasy - 博客园_files/201101192226374060.png" alt="image" width="164" height="39" border="0"></a>&nbsp;&nbsp;&nbsp; 这样就从一个m行的矩阵压缩到一个r行的矩阵了，对SVD来说也是一样的，我们对SVD分解的式子两边乘以U的转置U'</p>
<p><a href="http://images.cnblogs.com/cnblogs_com/LeftNotEasy/201101/201101192226377440.png"><img style="display: block; float: none; margin-left: auto; margin-right: auto; border-width: 0px;" title="image" src="./机器学习中的数学(5)-强大的矩阵奇异值分解(SVD)及其应用 - LeftNotEasy - 博客园_files/201101192226374408.png" alt="image" width="240" height="51" border="0"></a>&nbsp;&nbsp;&nbsp; 这样我们就得到了对行进行压缩的式子。可以看出，其实PCA几乎可以说是对SVD的一个包装，如果我们实现了SVD，那也就实现了PCA了，而且更好的地方是，有了SVD，我们就可以得到两个方向的PCA，如果我们对A’A进行特征值的分解，只能得到一个方向的PCA。</p>
<p>&nbsp;</p>
<p><span style="color: #0080ff; font-size: large;"><strong>四、奇异值与潜在语义索引LSI：</strong></span></p>
<p>&nbsp;&nbsp;&nbsp;&nbsp; 潜在语义索引（Latent Semantic Indexing）与PCA不太一样，至少不是实现了SVD就可以直接用的，不过LSI也是一个严重依赖于SVD的算法，之前吴军老师在<a href="http://www.google.com.hk/ggblog/googlechinablog/2006/12/blog-post_8935.html">矩阵计算与文本处理中的分类问题</a>中谈到：</p>
<p><em><span style="font-size: large;">&nbsp;&nbsp;&nbsp; “三个矩阵有非常清楚的物理含义。第一个矩阵X中的每一行表示意思相关的一类词，其中的每个非零元素表示这类词中每个词的重要性（或者说相关性），数值越大越相关。最后一个矩阵Y中的每一列表示同一主题一类文章，其中每个元素表示这类文章中每篇文章的相关性。中间的矩阵则表示类词和文章雷之间的相关性。因此，我们只要对关联矩阵A进行一次奇异值分解，w 我们就可以同时完成了近义词分类和文章的分类。（同时得到每类文章和每类词的相关性）。”</span></em></p>
<p>&nbsp;&nbsp;&nbsp;&nbsp; 上面这段话可能不太容易理解，不过这就是LSI的精髓内容，我下面举一个例子来说明一下，下面的例子来自LSA tutorial，具体的网址我将在最后的引用中给出：</p>
<p><a href="http://images.cnblogs.com/cnblogs_com/LeftNotEasy/201101/201101192226379109.png"><img style="display: block; float: none; margin-left: auto; margin-right: auto; border-width: 0px;" title="image" src="./机器学习中的数学(5)-强大的矩阵奇异值分解(SVD)及其应用 - LeftNotEasy - 博客园_files/201101192226386634.png" alt="image" width="316" height="285" border="0"></a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 这就是一个矩阵，不过不太一样的是，这里的一行表示一个词在哪些title中出现了（一行就是之前说的一维feature），一列表示一个title中有哪些词，（这个矩阵其实是我们之前说的那种一行是一个sample的形式的一种转置，这个会使得我们的左右奇异向量的意义产生变化，但是不会影响我们计算的过程）。比如说T1这个title中就有guide、investing、market、stock四个词，各出现了一次，我们将这个矩阵进行SVD，得到下面的矩阵：</p>
<p><a href="http://images.cnblogs.com/cnblogs_com/LeftNotEasy/201101/201101192226389491.png"><img style="display: block; float: none; margin-left: auto; margin-right: auto; border-width: 0px;" title="image" src="./机器学习中的数学(5)-强大的矩阵奇异值分解(SVD)及其应用 - LeftNotEasy - 博客园_files/201101192226397148.png" alt="image" width="624" height="266" border="0"></a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 左奇异向量表示词的一些特性，右奇异向量表示文档的一些特性，中间的奇异值矩阵表示左奇异向量的一行与右奇异向量的一列的重要程序，数字越大越重要。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 继续看这个矩阵还可以发现一些有意思的东西，首先，左奇异向量的第一列表示每一个词的出现频繁程度，虽然不是线性的，但是可以认为是一个大概的描述，比如book是0.15对应文档中出现的2次，investing是0.74对应了文档中出现了9次，rich是0.36对应文档中出现了3次；</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 其次，右奇异向量中一的第一行表示每一篇文档中的出现词的个数的近似，比如说，T6是0.49，出现了5个词，T2是0.22，出现了2个词。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 然后我们反过头来看，我们可以将左奇异向量和右奇异向量都取后2维（之前是3维的矩阵），投影到一个平面上，可以得到：</p>
<p><a href="http://images.cnblogs.com/cnblogs_com/LeftNotEasy/201101/20110119222640769.png"><img style="display: block; float: none; margin-left: auto; margin-right: auto; border-width: 0px;" title="image" src="./机器学习中的数学(5)-强大的矩阵奇异值分解(SVD)及其应用 - LeftNotEasy - 博客园_files/201101192226404739.png" alt="image" width="497" height="390" border="0"></a>&nbsp;&nbsp;&nbsp;&nbsp; 在图上，每一个红色的点，都表示一个词，每一个蓝色的点，都表示一篇文档，这样我们可以对这些词和文档进行聚类，比如说stock 和 market可以放在一类，因为他们老是出现在一起，real和estate可以放在一类，dads，guide这种词就看起来有点孤立了，我们就不对他们进行合并了。按这样聚类出现的效果，可以提取文档集合中的近义词，这样当用户检索文档的时候，是用语义级别（近义词集合）去检索了，而不是之前的词的级别。这样一减少我们的检索、存储量，因为这样压缩的文档集合和PCA是异曲同工的，二可以提高我们的用户体验，用户输入一个词，我们可以在这个词的近义词的集合中去找，这是传统的索引无法做到的。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp; 不知道按这样描述，再看看吴军老师的文章，是不是对SVD更清楚了？:-D</p>
<p>&nbsp;</p>
<p><span style="color: #0080ff; font-size: large;"><strong>参考资料：</strong></span></p>
<p>1）A Tutorial on Principal Component Analysis, Jonathon Shlens <br>&nbsp;&nbsp;&nbsp;&nbsp; 这是我关于用SVD去做PCA的主要参考资料    <br>2）<a href="http://www.ams.org/samplings/feature-column/fcarc-svd">http://www.ams.org/samplings/feature-column/fcarc-svd</a>    <br>&nbsp;&nbsp;&nbsp;&nbsp; 关于svd的一篇概念好文，我开头的几个图就是从这儿截取的    <br>3）<a href="http://www.puffinwarellc.com/index.php/news-and-articles/articles/30-singular-value-decomposition-tutorial.html">http://www.puffinwarellc.com/index.php/news-and-articles/articles/30-singular-value-decomposition-tutorial.html</a>    <br>&nbsp;&nbsp;&nbsp;&nbsp; 另一篇关于svd的入门好文    <br>4）<a href="http://www.puffinwarellc.com/index.php/news-and-articles/articles/33-latent-semantic-analysis-tutorial.html">http://www.puffinwarellc.com/index.php/news-and-articles/articles/33-latent-semantic-analysis-tutorial.html</a>    <br>&nbsp;&nbsp;&nbsp;&nbsp; svd与LSI的好文，我后面LSI中例子就是来自此    <br>5）<a href="http://www.miislita.com/information-retrieval-tutorial/svd-lsi-tutorial-1-understanding.html">http://www.miislita.com/information-retrieval-tutorial/svd-lsi-tutorial-1-understanding.html</a>    <br>&nbsp;&nbsp;&nbsp;&nbsp; 另一篇svd与LSI的文章，也还是不错，深一点，也比较长    <br>6）Singular Value Decomposition and Principal Component Analysis, Rasmus Elsborg Madsen, Lars Kai Hansen and Ole Winther, 2004    <br>&nbsp;&nbsp;&nbsp;&nbsp; 跟1）里面的文章比较类似</p>
</div>
<div id="MySignature"></div>
<div class="clear"></div>
<div id="blog_post_info_block"><div id="BlogPostCategory">
    分类: 
            <a href="https://www.cnblogs.com/LeftNotEasy/category/273622.html" target="_blank">机器学习</a>,             <a href="https://www.cnblogs.com/LeftNotEasy/category/273623.html" target="_blank">数学</a></div>
<div id="EntryTag">
    标签: 
            <a href="https://www.cnblogs.com/LeftNotEasy/tag/%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E/">搜索引擎</a>,             <a href="https://www.cnblogs.com/LeftNotEasy/tag/machine%20learning/">machine learning</a>,             <a href="https://www.cnblogs.com/LeftNotEasy/tag/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a>,             <a href="https://www.cnblogs.com/LeftNotEasy/tag/PCA/">PCA</a>,             <a href="https://www.cnblogs.com/LeftNotEasy/tag/SVD/">SVD</a>,             <a href="https://www.cnblogs.com/LeftNotEasy/tag/LSA/">LSA</a>,             <a href="https://www.cnblogs.com/LeftNotEasy/tag/LSI/">LSI</a></div>

    <div id="blog_post_info">
<div id="green_channel">
        <a href="javascript:void(0);" id="green_channel_digg" onclick="DiggIt(1939687,cb_blogId,1);green_channel_success(this,&#39;谢谢推荐！&#39;);">好文要顶</a>
        <a id="green_channel_follow" onclick="follow(&#39;e78b6fc1-4fa4-de11-ba8f-001cf0cd104b&#39;);" href="javascript:void(0);">关注我</a>
    <a id="green_channel_favorite" onclick="AddToWz(cb_entryId);return false;" href="javascript:void(0);">收藏该文</a>
    <a id="green_channel_weibo" href="javascript:void(0);" title="分享至新浪微博" onclick="ShareToTsina()"><img src="./机器学习中的数学(5)-强大的矩阵奇异值分解(SVD)及其应用 - LeftNotEasy - 博客园_files/icon_weibo_24.png" alt=""></a>
    <a id="green_channel_wechat" href="javascript:void(0);" title="分享至微信" onclick="shareOnWechat()"><img src="./机器学习中的数学(5)-强大的矩阵奇异值分解(SVD)及其应用 - LeftNotEasy - 博客园_files/wechat.png" alt=""></a>
</div>
<div id="author_profile">
    <div id="author_profile_info" class="author_profile_info">
            <a href="https://home.cnblogs.com/u/LeftNotEasy/" target="_blank"><img src="./机器学习中的数学(5)-强大的矩阵奇异值分解(SVD)及其应用 - LeftNotEasy - 博客园_files/u89123.png" class="author_avatar" alt=""></a>
        <div id="author_profile_detail" class="author_profile_info">
            <a href="https://home.cnblogs.com/u/LeftNotEasy/">LeftNotEasy</a><br>
            <a href="https://home.cnblogs.com/u/LeftNotEasy/followees/">关注 - 16</a><br>
            <a href="https://home.cnblogs.com/u/LeftNotEasy/followers/">粉丝 - 1586</a>
        </div>
    </div>
    <div class="clear"></div>
    <div id="author_profile_honor"></div>
    <div id="author_profile_follow">
                <a href="javascript:void(0);" onclick="follow(&#39;e78b6fc1-4fa4-de11-ba8f-001cf0cd104b&#39;);return false;">+加关注</a>
    </div>
</div>
<div id="div_digg">
    <div class="diggit" onclick="votePost(1939687,&#39;Digg&#39;)">
        <span class="diggnum" id="digg_count">119</span>
    </div>
    <div class="buryit" onclick="votePost(1939687,&#39;Bury&#39;)">
        <span class="burynum" id="bury_count">2</span>
    </div>
    <div class="clear"></div>
    <div class="diggword" id="digg_tips">
    </div>
</div>

<script type="text/javascript">
    currentDiggType = 0;
</script></div>
    <div class="clear"></div>
    <div id="post_next_prev">

    <a href="https://www.cnblogs.com/LeftNotEasy/archive/2011/01/08/lda-and-pca-machine-learning.html" class="p_n_p_prefix">« </a> 上一篇：    <a href="https://www.cnblogs.com/LeftNotEasy/archive/2011/01/08/lda-and-pca-machine-learning.html" title="发布于 2011-01-08 14:56">机器学习中的数学(4)-线性判别分析（LDA）, 主成分分析(PCA)</a>
    <br>
    <a href="https://www.cnblogs.com/LeftNotEasy/archive/2011/02/27/py_mining_first_release.html" class="p_n_p_prefix">» </a> 下一篇：    <a href="https://www.cnblogs.com/LeftNotEasy/archive/2011/02/27/py_mining_first_release.html" title="发布于 2011-02-27 14:33">支持中文文本的数据挖掘平台开源项目PyMining发布</a>

</div>
</div>
            </div>
            <div class="postDesc">posted @ 
<span id="post-date">2011-01-19 22:27</span>&nbsp;<a href="https://www.cnblogs.com/LeftNotEasy/">LeftNotEasy</a> 阅读(<span id="post_view_count">349250</span>) 评论(<span id="post_comment_count">81</span>) <a href="https://i.cnblogs.com/EditPosts.aspx?postid=1939687" rel="nofollow"> 编辑</a> <a href="javascript:void(0)" onclick="AddToWz(1939687); return false;">收藏</a>
</div>
        </div>
	    
	    
    </div><!--end: topics 文章、评论容器-->
</div>
<script src="./机器学习中的数学(5)-强大的矩阵奇异值分解(SVD)及其应用 - LeftNotEasy - 博客园_files/highlight.min.js"></script>
<script>markdown_highlight();</script>
<script>
    var allowComments = true, cb_blogId = 62514, cb_blogApp = 'LeftNotEasy', cb_blogUserGuid = 'e78b6fc1-4fa4-de11-ba8f-001cf0cd104b';
    var cb_entryId = 1939687, cb_entryCreatedDate = '2011-01-19 22:27', cb_postType = 1; 
    loadViewCount(cb_entryId);
</script><a name="!comments"></a>
<div id="blog-comments-placeholder">

<div id="comment_pager_top">
    
    <div class="pager">
        <!-- 上一页 -->
        <a href="https://www.cnblogs.com/LeftNotEasy/archive/2011/01/19/svd-and-applications.html#!comments" onclick="commentManager.renderComments(1, 50); return false;">
            &lt; Prev
        </a>

        <!-- 第一页 -->
        <a href="https://www.cnblogs.com/LeftNotEasy/archive/2011/01/19/svd-and-applications.html#!comments" onclick="commentManager.renderComments(1, 50); return false;">
            1
        </a>

        <!-- 前半部分页码 -->
        

        <!-- 当前页 -->
        <span class="current">2</span>

        <!-- 后半部分页码 -->

        
        <!-- 末尾页 -->
        

        <!-- 下一页 -->
        
    </div>
</div>

<br>
<div class="feedback_area_title">评论列表</div>
<div class="feedbackNoItems"><div class="feedbackNoItems"></div></div>	
		<div class="feedbackItem">
			<div class="feedbackListSubtitle">
				<div class="feedbackManage">
					&nbsp;&nbsp;

<span class="comment_actions">
    
    
    
    
</span>


				</div>
				
<a href="https://www.cnblogs.com/LeftNotEasy/archive/2011/01/19/svd-and-applications.html#3141257" class="layer">#51楼</a>
<a name="3141257" id="comment_anchor_3141257"></a>

 
<span class="comment_date">2015-03-15 19:01</span>

 

            <a id="a_comment_author_3141257" href="https://home.cnblogs.com/u/659659/" target="_blank">pluszero</a>

			</div>
			<div class="feedbackCon">
				
<div id="comment_body_3141257" class="blog_comment_body">
    404error。。。
</div>
        <div class="comment_vote">
            <a href="javascript:void(0);" class="comment_digg" onclick="return voteComment(&#39;3141257&#39;, &#39;Digg&#39;, this);">
                支持(1)
            </a>
            <a href="javascript:void(0);" class="comment_burry" onclick="return voteComment(&#39;3141257&#39;, &#39;Bury&#39;, this);">
                反对(0)
            </a>
        </div>
        

			</div>
		</div>
		<div class="feedbackItem">
			<div class="feedbackListSubtitle">
				<div class="feedbackManage">
					&nbsp;&nbsp;

<span class="comment_actions">
    
    
    
    
</span>


				</div>
				
<a href="https://www.cnblogs.com/LeftNotEasy/archive/2011/01/19/svd-and-applications.html#3153499" class="layer">#52楼</a>
<a name="3153499" id="comment_anchor_3153499"></a>

 
<span class="comment_date">2015-03-31 15:57</span>

 

            <a id="a_comment_author_3153499" href="https://home.cnblogs.com/u/738150/" target="_blank">一晌贪欢</a>

			</div>
			<div class="feedbackCon">
				
<div id="comment_body_3153499" class="blog_comment_body">
    好文，一目了然
</div>
        <div class="comment_vote">
            <a href="javascript:void(0);" class="comment_digg" onclick="return voteComment(&#39;3153499&#39;, &#39;Digg&#39;, this);">
                支持(0)
            </a>
            <a href="javascript:void(0);" class="comment_burry" onclick="return voteComment(&#39;3153499&#39;, &#39;Bury&#39;, this);">
                反对(1)
            </a>
        </div>
        

			</div>
		</div>
		<div class="feedbackItem">
			<div class="feedbackListSubtitle">
				<div class="feedbackManage">
					&nbsp;&nbsp;

<span class="comment_actions">
    
    
    
    
</span>


				</div>
				
<a href="https://www.cnblogs.com/LeftNotEasy/archive/2011/01/19/svd-and-applications.html#3225222" class="layer">#53楼</a>
<a name="3225222" id="comment_anchor_3225222"></a>

 
<span class="comment_date">2015-07-10 10:38</span>

 

            <a id="a_comment_author_3225222" href="https://home.cnblogs.com/u/753794/" target="_blank">haors_OD</a>

			</div>
			<div class="feedbackCon">
				
<div id="comment_body_3225222" class="blog_comment_body">
    好文，赞
</div>
        <div class="comment_vote">
            <a href="javascript:void(0);" class="comment_digg" onclick="return voteComment(&#39;3225222&#39;, &#39;Digg&#39;, this);">
                支持(2)
            </a>
            <a href="javascript:void(0);" class="comment_burry" onclick="return voteComment(&#39;3225222&#39;, &#39;Bury&#39;, this);">
                反对(0)
            </a>
        </div>
        

			</div>
		</div>
		<div class="feedbackItem">
			<div class="feedbackListSubtitle">
				<div class="feedbackManage">
					&nbsp;&nbsp;

<span class="comment_actions">
    
    
    
    
</span>


				</div>
				
<a href="https://www.cnblogs.com/LeftNotEasy/archive/2011/01/19/svd-and-applications.html#3227315" class="layer">#54楼</a>
<a name="3227315" id="comment_anchor_3227315"></a>

 
<span class="comment_date">2015-07-14 09:59</span>

 

            <a id="a_comment_author_3227315" href="https://www.cnblogs.com/CheeseZH/" target="_blank">ZH奶酪</a>

			</div>
			<div class="feedbackCon">
				
<div id="comment_body_3227315" class="blog_comment_body">
    赞~
</div>
        <div class="comment_vote">
            <a href="javascript:void(0);" class="comment_digg" onclick="return voteComment(&#39;3227315&#39;, &#39;Digg&#39;, this);">
                支持(1)
            </a>
            <a href="javascript:void(0);" class="comment_burry" onclick="return voteComment(&#39;3227315&#39;, &#39;Bury&#39;, this);">
                反对(0)
            </a>
        </div>
        <span id="comment_3227315_avatar" style="display:none">
            https://pic.cnblogs.com/face/323808/20150421190230.png
        </span>

			</div>
		</div>
		<div class="feedbackItem">
			<div class="feedbackListSubtitle">
				<div class="feedbackManage">
					&nbsp;&nbsp;

<span class="comment_actions">
    
    
    
    
</span>


				</div>
				
<a href="https://www.cnblogs.com/LeftNotEasy/archive/2011/01/19/svd-and-applications.html#3287974" class="layer">#55楼</a>
<a name="3287974" id="comment_anchor_3287974"></a>

 
<span class="comment_date">2015-10-20 03:48</span>

 

            <a id="a_comment_author_3287974" href="https://home.cnblogs.com/u/322226/" target="_blank">rujingyu</a>

			</div>
			<div class="feedbackCon">
				
<div id="comment_body_3287974" class="blog_comment_body">
    难得的好文，希望楼主继续更新。
</div>
        <div class="comment_vote">
            <a href="javascript:void(0);" class="comment_digg" onclick="return voteComment(&#39;3287974&#39;, &#39;Digg&#39;, this);">
                支持(0)
            </a>
            <a href="javascript:void(0);" class="comment_burry" onclick="return voteComment(&#39;3287974&#39;, &#39;Bury&#39;, this);">
                反对(0)
            </a>
        </div>
        

			</div>
		</div>
		<div class="feedbackItem">
			<div class="feedbackListSubtitle">
				<div class="feedbackManage">
					&nbsp;&nbsp;

<span class="comment_actions">
    
    
    
    
</span>


				</div>
				
<a href="https://www.cnblogs.com/LeftNotEasy/archive/2011/01/19/svd-and-applications.html#3329430" class="layer">#56楼</a>
<a name="3329430" id="comment_anchor_3329430"></a>

 
<span class="comment_date">2015-12-18 10:28</span>

 

            <a id="a_comment_author_3329430" href="https://www.cnblogs.com/lifesider/" target="_blank">走在人生边上</a>

			</div>
			<div class="feedbackCon">
				
<div id="comment_body_3329430" class="blog_comment_body">
    写的很棒！
</div>
        <div class="comment_vote">
            <a href="javascript:void(0);" class="comment_digg" onclick="return voteComment(&#39;3329430&#39;, &#39;Digg&#39;, this);">
                支持(1)
            </a>
            <a href="javascript:void(0);" class="comment_burry" onclick="return voteComment(&#39;3329430&#39;, &#39;Bury&#39;, this);">
                反对(0)
            </a>
        </div>
        <span id="comment_3329430_avatar" style="display:none">
            https://pic.cnblogs.com/face/554120/20170515165659.png
        </span>

			</div>
		</div>
		<div class="feedbackItem">
			<div class="feedbackListSubtitle">
				<div class="feedbackManage">
					&nbsp;&nbsp;

<span class="comment_actions">
    
    
    
    
</span>


				</div>
				
<a href="https://www.cnblogs.com/LeftNotEasy/archive/2011/01/19/svd-and-applications.html#3333478" class="layer">#57楼</a>
<a name="3333478" id="comment_anchor_3333478"></a>

 
<span class="comment_date">2015-12-24 11:26</span>

 

            <a id="a_comment_author_3333478" href="https://home.cnblogs.com/u/864469/" target="_blank">他们是青年</a>

			</div>
			<div class="feedbackCon">
				
<div id="comment_body_3333478" class="blog_comment_body">
    很好！！谢谢
</div>
        <div class="comment_vote">
            <a href="javascript:void(0);" class="comment_digg" onclick="return voteComment(&#39;3333478&#39;, &#39;Digg&#39;, this);">
                支持(0)
            </a>
            <a href="javascript:void(0);" class="comment_burry" onclick="return voteComment(&#39;3333478&#39;, &#39;Bury&#39;, this);">
                反对(0)
            </a>
        </div>
        

			</div>
		</div>
		<div class="feedbackItem">
			<div class="feedbackListSubtitle">
				<div class="feedbackManage">
					&nbsp;&nbsp;

<span class="comment_actions">
    
    
    
    
</span>


				</div>
				
<a href="https://www.cnblogs.com/LeftNotEasy/archive/2011/01/19/svd-and-applications.html#3421089" class="layer">#58楼</a>
<a name="3421089" id="comment_anchor_3421089"></a>

 
<span class="comment_date">2016-05-02 11:55</span>

 

            <a id="a_comment_author_3421089" href="https://www.cnblogs.com/goingmyway/" target="_blank">GoingMyWay</a>

			</div>
			<div class="feedbackCon">
				
<div id="comment_body_3421089" class="blog_comment_body">
    看懂了，谢谢！
</div>
        <div class="comment_vote">
            <a href="javascript:void(0);" class="comment_digg" onclick="return voteComment(&#39;3421089&#39;, &#39;Digg&#39;, this);">
                支持(0)
            </a>
            <a href="javascript:void(0);" class="comment_burry" onclick="return voteComment(&#39;3421089&#39;, &#39;Bury&#39;, this);">
                反对(0)
            </a>
        </div>
        

			</div>
		</div>
		<div class="feedbackItem">
			<div class="feedbackListSubtitle">
				<div class="feedbackManage">
					&nbsp;&nbsp;

<span class="comment_actions">
    
    
    
    
</span>


				</div>
				
<a href="https://www.cnblogs.com/LeftNotEasy/archive/2011/01/19/svd-and-applications.html#3427993" class="layer">#59楼</a>
<a name="3427993" id="comment_anchor_3427993"></a>

 
<span class="comment_date">2016-05-10 20:15</span>

 

            <a id="a_comment_author_3427993" href="https://home.cnblogs.com/u/953844/" target="_blank">echo_TJ</a>

			</div>
			<div class="feedbackCon">
				
<div id="comment_body_3427993" class="blog_comment_body">
    博主，PCA中利用特征值进行降维时对象只局限于方阵？？？
</div>
        <div class="comment_vote">
            <a href="javascript:void(0);" class="comment_digg" onclick="return voteComment(&#39;3427993&#39;, &#39;Digg&#39;, this);">
                支持(0)
            </a>
            <a href="javascript:void(0);" class="comment_burry" onclick="return voteComment(&#39;3427993&#39;, &#39;Bury&#39;, this);">
                反对(0)
            </a>
        </div>
        

			</div>
		</div>
		<div class="feedbackItem">
			<div class="feedbackListSubtitle">
				<div class="feedbackManage">
					&nbsp;&nbsp;

<span class="comment_actions">
    
    
    
    
</span>


				</div>
				
<a href="https://www.cnblogs.com/LeftNotEasy/archive/2011/01/19/svd-and-applications.html#3435751" class="layer">#60楼</a>
<a name="3435751" id="comment_anchor_3435751"></a>

 
<span class="comment_date">2016-05-20 14:31</span>

 

            <a id="a_comment_author_3435751" href="https://www.cnblogs.com/Stomach-ache/" target="_blank">Stomach_ache</a>

			</div>
			<div class="feedbackCon">
				
<div id="comment_body_3435751" class="blog_comment_body">
    <a href="https://www.cnblogs.com/LeftNotEasy/archive/2011/01/19/svd-and-applications.html#3427993" title="查看所回复的评论" onclick="commentManager.renderComments(0,50,3427993);">@</a>
echo_TJ<br>当然不是。PCA需要求协方差矩阵，协方差矩阵就是方阵。
</div>
        <div class="comment_vote">
            <a href="javascript:void(0);" class="comment_digg" onclick="return voteComment(&#39;3435751&#39;, &#39;Digg&#39;, this);">
                支持(0)
            </a>
            <a href="javascript:void(0);" class="comment_burry" onclick="return voteComment(&#39;3435751&#39;, &#39;Bury&#39;, this);">
                反对(0)
            </a>
        </div>
        <span id="comment_3435751_avatar" style="display:none">
            https://pic.cnblogs.com/face/629466/20140501093735.png
        </span>

			</div>
		</div>
		<div class="feedbackItem">
			<div class="feedbackListSubtitle">
				<div class="feedbackManage">
					&nbsp;&nbsp;

<span class="comment_actions">
    
    
    
    
</span>


				</div>
				
<a href="https://www.cnblogs.com/LeftNotEasy/archive/2011/01/19/svd-and-applications.html#3482273" class="layer">#61楼</a>
<a name="3482273" id="comment_anchor_3482273"></a>

 
<span class="comment_date">2016-08-03 16:05</span>

 

            <a id="a_comment_author_3482273" href="https://home.cnblogs.com/u/995868/" target="_blank">i_See</a>

			</div>
			<div class="feedbackCon">
				
<div id="comment_body_3482273" class="blog_comment_body">
    “假设A是一个N * M的矩阵，那么得到的U是一个N * N的方阵”<br>博主在介绍“奇异值”的假设与下面的图片不太一致，对于只有一点线代基础的我有点晕。不过查了相关资料还是搞明白。谢谢博主的分享。
</div>
        <div class="comment_vote">
            <a href="javascript:void(0);" class="comment_digg" onclick="return voteComment(&#39;3482273&#39;, &#39;Digg&#39;, this);">
                支持(1)
            </a>
            <a href="javascript:void(0);" class="comment_burry" onclick="return voteComment(&#39;3482273&#39;, &#39;Bury&#39;, this);">
                反对(0)
            </a>
        </div>
        

			</div>
		</div>
		<div class="feedbackItem">
			<div class="feedbackListSubtitle">
				<div class="feedbackManage">
					&nbsp;&nbsp;

<span class="comment_actions">
    
    
    
    
</span>


				</div>
				
<a href="https://www.cnblogs.com/LeftNotEasy/archive/2011/01/19/svd-and-applications.html#3526489" class="layer">#62楼</a>
<a name="3526489" id="comment_anchor_3526489"></a>

 
<span class="comment_date">2016-10-09 15:55</span>

 

            <a id="a_comment_author_3526489" href="https://home.cnblogs.com/u/1038680/" target="_blank">hope2016</a>

			</div>
			<div class="feedbackCon">
				
<div id="comment_body_3526489" class="blog_comment_body">
    感谢楼主分享，干货，赞！<br><br>不过，对于特征值分解这一段，有几个细节不太严格（可能是楼主篇幅所限，没有严格讲清晰），有两点：一是待分解的矩阵A需是对称矩阵，否则A不能相似于一个对角阵西格玛。二是原文提到：“其中Q是这个矩阵A的特征向量组成的矩阵”，这里的Q不是任意一个特征向量就可以构成的，而是必须是由某特殊的特征向量才能构成，比如要求限制该特征向量模为1或者要求Q是正交矩阵等。<br><br>最后，本文瑕不掩瑜，支持一下！
</div>
        <div class="comment_vote">
            <a href="javascript:void(0);" class="comment_digg" onclick="return voteComment(&#39;3526489&#39;, &#39;Digg&#39;, this);">
                支持(1)
            </a>
            <a href="javascript:void(0);" class="comment_burry" onclick="return voteComment(&#39;3526489&#39;, &#39;Bury&#39;, this);">
                反对(0)
            </a>
        </div>
        

			</div>
		</div>
		<div class="feedbackItem">
			<div class="feedbackListSubtitle">
				<div class="feedbackManage">
					&nbsp;&nbsp;

<span class="comment_actions">
    
    
    
    
</span>


				</div>
				
<a href="https://www.cnblogs.com/LeftNotEasy/archive/2011/01/19/svd-and-applications.html#3538988" class="layer">#63楼</a>
<a name="3538988" id="comment_anchor_3538988"></a>

 
<span class="comment_date">2016-10-23 19:26</span>

 

            <a id="a_comment_author_3538988" href="https://www.cnblogs.com/caidongzhou/" target="_blank">生活它不如诗</a>

			</div>
			<div class="feedbackCon">
				
<div id="comment_body_3538988" class="blog_comment_body">
    我有一个问题。望解答，谢谢。<br>在潜在语义索引这个例子中，分解后，左边矩阵为词的矩阵。每一列代表一维（假设名字叫A），就构成了一个三维（A,B,C)的空间。<br>问题1. 每一列有11个值，是不是意味着A这个维度又由11个维度组成（即11个数值才能表示）？<br>问题2. 还是以词这个矩阵为例，我们考虑第一行，book 0.15 -0.27 0.04 意味着在这个三维空间中，book一词在这三个维度上的映射，即表示book在这个空间中的三个抽象概念中的大小（说法不是很准确，理解为相关度或者权重，如0.15其实表示词频），那么就有一个问题。一方面每个数值要表示某个概念上的权重，另一方面，它要与同一列中剩下的10个值一起构建一个维度。这两个方面应该是各自独立的，但是现在一个值要肩负两方面的表示。我感觉是我理解错了，能否解释一下？<br>非常感谢。
</div>
        <div class="comment_vote">
            <a href="javascript:void(0);" class="comment_digg" onclick="return voteComment(&#39;3538988&#39;, &#39;Digg&#39;, this);">
                支持(0)
            </a>
            <a href="javascript:void(0);" class="comment_burry" onclick="return voteComment(&#39;3538988&#39;, &#39;Bury&#39;, this);">
                反对(0)
            </a>
        </div>
        <span id="comment_3538988_avatar" style="display:none">
            https://pic.cnblogs.com/face/925831/20160514213851.png
        </span>

			</div>
		</div>
		<div class="feedbackItem">
			<div class="feedbackListSubtitle">
				<div class="feedbackManage">
					&nbsp;&nbsp;

<span class="comment_actions">
    
    
    
    
</span>


				</div>
				
<a href="https://www.cnblogs.com/LeftNotEasy/archive/2011/01/19/svd-and-applications.html#3563159" class="layer">#64楼</a>
<a name="3563159" id="comment_anchor_3563159"></a>

 
<span class="comment_date">2016-11-23 16:03</span>

 

            <a id="a_comment_author_3563159" href="https://home.cnblogs.com/u/1068684/" target="_blank">Sophie冯</a>

			</div>
			<div class="feedbackCon">
				
<div id="comment_body_3563159" class="blog_comment_body">
    好文章。不过文中这句话“一个矩阵其实就是一个线性变换”有问题，2*2的矩阵是线性变换，但是3*3的矩阵就是仿射变换甚至透视变换，线性变换只是高阶矩阵的一个特例。
</div>
        <div class="comment_vote">
            <a href="javascript:void(0);" class="comment_digg" onclick="return voteComment(&#39;3563159&#39;, &#39;Digg&#39;, this);">
                支持(0)
            </a>
            <a href="javascript:void(0);" class="comment_burry" onclick="return voteComment(&#39;3563159&#39;, &#39;Bury&#39;, this);">
                反对(1)
            </a>
        </div>
        

			</div>
		</div>
		<div class="feedbackItem">
			<div class="feedbackListSubtitle">
				<div class="feedbackManage">
					&nbsp;&nbsp;

<span class="comment_actions">
    
    
    
    
</span>


				</div>
				
<a href="https://www.cnblogs.com/LeftNotEasy/archive/2011/01/19/svd-and-applications.html#3571476" class="layer">#65楼</a>
<a name="3571476" id="comment_anchor_3571476"></a>

 
<span class="comment_date">2016-12-03 21:04</span>

 

            <a id="a_comment_author_3571476" href="https://home.cnblogs.com/u/1074568/" target="_blank">wyxdsh</a>

			</div>
			<div class="feedbackCon">
				
<div id="comment_body_3571476" class="blog_comment_body">
    你好，一直不太懂SVD可以分解任何矩阵，而PCA只能分解方阵。假如我们在实际数据分析过程中有一个m*n的数据矩阵，列是变量，行是样本。所以用PCA分解的方阵是协方差矩阵？？？那如果用SVD呢，分解的是原始矩阵？？？<br>还有，用SVD可以估计模型参数吗？<br>谢谢啦
</div>
        <div class="comment_vote">
            <a href="javascript:void(0);" class="comment_digg" onclick="return voteComment(&#39;3571476&#39;, &#39;Digg&#39;, this);">
                支持(0)
            </a>
            <a href="javascript:void(0);" class="comment_burry" onclick="return voteComment(&#39;3571476&#39;, &#39;Bury&#39;, this);">
                反对(0)
            </a>
        </div>
        

			</div>
		</div>
		<div class="feedbackItem">
			<div class="feedbackListSubtitle">
				<div class="feedbackManage">
					&nbsp;&nbsp;

<span class="comment_actions">
    
    
    
    
</span>


				</div>
				
<a href="https://www.cnblogs.com/LeftNotEasy/archive/2011/01/19/svd-and-applications.html#3594475" class="layer">#66楼</a>
<a name="3594475" id="comment_anchor_3594475"></a>

 
<span class="comment_date">2017-01-02 15:31</span>

 

            <a id="a_comment_author_3594475" href="https://www.cnblogs.com/wft1990/" target="_blank">泡面小王子</a>

			</div>
			<div class="feedbackCon">
				
<div id="comment_body_3594475" class="blog_comment_body">
    2 奇异值部分<br>假设A是一个N * M的矩阵，那么得到的U是一个N * N的方阵，Σ是一个N * M的矩阵，<b>V’(V的转置)是一个N * N的矩阵</b> 这个地方是错的 应该是 V'=M*M
</div>
        <div class="comment_vote">
            <a href="javascript:void(0);" class="comment_digg" onclick="return voteComment(&#39;3594475&#39;, &#39;Digg&#39;, this);">
                支持(2)
            </a>
            <a href="javascript:void(0);" class="comment_burry" onclick="return voteComment(&#39;3594475&#39;, &#39;Bury&#39;, this);">
                反对(0)
            </a>
        </div>
        <span id="comment_3594475_avatar" style="display:none">
            https://pic.cnblogs.com/face/724459/20150222194151.png
        </span>

			</div>
		</div>
		<div class="feedbackItem">
			<div class="feedbackListSubtitle">
				<div class="feedbackManage">
					&nbsp;&nbsp;

<span class="comment_actions">
    
    
    
    
</span>


				</div>
				
<a href="https://www.cnblogs.com/LeftNotEasy/archive/2011/01/19/svd-and-applications.html#3617253" class="layer">#67楼</a>
<a name="3617253" id="comment_anchor_3617253"></a>

 
<span class="comment_date">2017-02-13 12:26</span>

 

            <a id="a_comment_author_3617253" href="https://www.cnblogs.com/kjs-linux/" target="_blank">小工匠</a>

			</div>
			<div class="feedbackCon">
				
<div id="comment_body_3617253" class="blog_comment_body">
    博主，我自学人工智能，有哪些书推荐一下吧
</div>
        <div class="comment_vote">
            <a href="javascript:void(0);" class="comment_digg" onclick="return voteComment(&#39;3617253&#39;, &#39;Digg&#39;, this);">
                支持(0)
            </a>
            <a href="javascript:void(0);" class="comment_burry" onclick="return voteComment(&#39;3617253&#39;, &#39;Bury&#39;, this);">
                反对(2)
            </a>
        </div>
        <span id="comment_3617253_avatar" style="display:none">
            https://pic.cnblogs.com/face/685755/20141026210238.png
        </span>

			</div>
		</div>
		<div class="feedbackItem">
			<div class="feedbackListSubtitle">
				<div class="feedbackManage">
					&nbsp;&nbsp;

<span class="comment_actions">
    
    
    
    
</span>


				</div>
				
<a href="https://www.cnblogs.com/LeftNotEasy/archive/2011/01/19/svd-and-applications.html#3647226" class="layer">#68楼</a>
<a name="3647226" id="comment_anchor_3647226"></a>

 
<span class="comment_date">2017-03-21 11:41</span>

 

            <a id="a_comment_author_3647226" href="https://home.cnblogs.com/u/1130726/" target="_blank">ReeJuly</a>

			</div>
			<div class="feedbackCon">
				
<div id="comment_body_3647226" class="blog_comment_body">
    好文先赞为敬
</div>
        <div class="comment_vote">
            <a href="javascript:void(0);" class="comment_digg" onclick="return voteComment(&#39;3647226&#39;, &#39;Digg&#39;, this);">
                支持(0)
            </a>
            <a href="javascript:void(0);" class="comment_burry" onclick="return voteComment(&#39;3647226&#39;, &#39;Bury&#39;, this);">
                反对(0)
            </a>
        </div>
        

			</div>
		</div>
		<div class="feedbackItem">
			<div class="feedbackListSubtitle">
				<div class="feedbackManage">
					&nbsp;&nbsp;

<span class="comment_actions">
    
    
    
    
</span>


				</div>
				
<a href="https://www.cnblogs.com/LeftNotEasy/archive/2011/01/19/svd-and-applications.html#3663591" class="layer">#69楼</a>
<a name="3663591" id="comment_anchor_3663591"></a>

 
<span class="comment_date">2017-04-09 21:50</span>

 

            <a id="a_comment_author_3663591" href="https://home.cnblogs.com/u/1128553/" target="_blank">muyeby</a>

			</div>
			<div class="feedbackCon">
				
<div id="comment_body_3663591" class="blog_comment_body">
    干货文，赞赞赞
</div>
        <div class="comment_vote">
            <a href="javascript:void(0);" class="comment_digg" onclick="return voteComment(&#39;3663591&#39;, &#39;Digg&#39;, this);">
                支持(0)
            </a>
            <a href="javascript:void(0);" class="comment_burry" onclick="return voteComment(&#39;3663591&#39;, &#39;Bury&#39;, this);">
                反对(0)
            </a>
        </div>
        

			</div>
		</div>
		<div class="feedbackItem">
			<div class="feedbackListSubtitle">
				<div class="feedbackManage">
					&nbsp;&nbsp;

<span class="comment_actions">
    
    
    
    
</span>


				</div>
				
<a href="https://www.cnblogs.com/LeftNotEasy/archive/2011/01/19/svd-and-applications.html#3793877" class="layer">#70楼</a>
<a name="3793877" id="comment_anchor_3793877"></a>

 
<span class="comment_date">2017-09-22 21:58</span>

 

            <a id="a_comment_author_3793877" href="https://www.cnblogs.com/dobestself-994395/" target="_blank">dobestself_994395</a>

			</div>
			<div class="feedbackCon">
				
<div id="comment_body_3793877" class="blog_comment_body">
    您有，有一个疑问再请教一下，一个矩阵的一组特征向量不一定是一组正交向量吧。比如对于矩阵[1,2,0;0,3,0;2,-4,2](每个分号代表一行)，它的特征向量是[-1,-1,2],[0,0,1],[-1,0,2]。这三个特征向量并不是一组正交向量。
</div>
        <div class="comment_vote">
            <a href="javascript:void(0);" class="comment_digg" onclick="return voteComment(&#39;3793877&#39;, &#39;Digg&#39;, this);">
                支持(0)
            </a>
            <a href="javascript:void(0);" class="comment_burry" onclick="return voteComment(&#39;3793877&#39;, &#39;Bury&#39;, this);">
                反对(0)
            </a>
        </div>
        <span id="comment_3793877_avatar" style="display:none">
            https://pic.cnblogs.com/face/718558/20150127225104.png
        </span>

			</div>
		</div>
		<div class="feedbackItem">
			<div class="feedbackListSubtitle">
				<div class="feedbackManage">
					&nbsp;&nbsp;

<span class="comment_actions">
    
    
    
    
</span>


				</div>
				
<a href="https://www.cnblogs.com/LeftNotEasy/archive/2011/01/19/svd-and-applications.html#3864614" class="layer">#71楼</a>
<a name="3864614" id="comment_anchor_3864614"></a>

 
<span class="comment_date">2017-12-13 11:09</span>

 

            <a id="a_comment_author_3864614" href="https://home.cnblogs.com/u/1294716/" target="_blank">大菜鸡</a>

			</div>
			<div class="feedbackCon">
				
<div id="comment_body_3864614" class="blog_comment_body">
    我懂了，非常好
</div>
        <div class="comment_vote">
            <a href="javascript:void(0);" class="comment_digg" onclick="return voteComment(&#39;3864614&#39;, &#39;Digg&#39;, this);">
                支持(0)
            </a>
            <a href="javascript:void(0);" class="comment_burry" onclick="return voteComment(&#39;3864614&#39;, &#39;Bury&#39;, this);">
                反对(0)
            </a>
        </div>
        

			</div>
		</div>
		<div class="feedbackItem">
			<div class="feedbackListSubtitle">
				<div class="feedbackManage">
					&nbsp;&nbsp;

<span class="comment_actions">
    
    
    
    
</span>


				</div>
				
<a href="https://www.cnblogs.com/LeftNotEasy/archive/2011/01/19/svd-and-applications.html#3872351" class="layer">#72楼</a>
<a name="3872351" id="comment_anchor_3872351"></a>

 
<span class="comment_date">2017-12-23 19:46</span>

 

            <a id="a_comment_author_3872351" href="https://www.cnblogs.com/xflqm/" target="_blank">dang幸福来敲门</a>

			</div>
			<div class="feedbackCon">
				
<div id="comment_body_3872351" class="blog_comment_body">
    写错了！转置！
</div>
        <div class="comment_vote">
            <a href="javascript:void(0);" class="comment_digg" onclick="return voteComment(&#39;3872351&#39;, &#39;Digg&#39;, this);">
                支持(0)
            </a>
            <a href="javascript:void(0);" class="comment_burry" onclick="return voteComment(&#39;3872351&#39;, &#39;Bury&#39;, this);">
                反对(0)
            </a>
        </div>
        

			</div>
		</div>
		<div class="feedbackItem">
			<div class="feedbackListSubtitle">
				<div class="feedbackManage">
					&nbsp;&nbsp;

<span class="comment_actions">
    
    
    
    
</span>


				</div>
				
<a href="https://www.cnblogs.com/LeftNotEasy/archive/2011/01/19/svd-and-applications.html#3905410" class="layer">#73楼</a>
<a name="3905410" id="comment_anchor_3905410"></a>

 
<span class="comment_date">2018-02-07 12:09</span>

 

            <a id="a_comment_author_3905410" href="https://home.cnblogs.com/u/1242141/" target="_blank">yingchen_wy</a>

			</div>
			<div class="feedbackCon">
				
<div id="comment_body_3905410" class="blog_comment_body">
    @i_See 请问，你是怎么把这个问题搞清楚的啊？我也是不明白为什么奇异值分解之后矩阵变成这个样子了，求指教
</div>
        <div class="comment_vote">
            <a href="javascript:void(0);" class="comment_digg" onclick="return voteComment(&#39;3905410&#39;, &#39;Digg&#39;, this);">
                支持(0)
            </a>
            <a href="javascript:void(0);" class="comment_burry" onclick="return voteComment(&#39;3905410&#39;, &#39;Bury&#39;, this);">
                反对(0)
            </a>
        </div>
        

			</div>
		</div>
		<div class="feedbackItem">
			<div class="feedbackListSubtitle">
				<div class="feedbackManage">
					&nbsp;&nbsp;

<span class="comment_actions">
    
    
    
    
</span>


				</div>
				
<a href="https://www.cnblogs.com/LeftNotEasy/archive/2011/01/19/svd-and-applications.html#3930150" class="layer">#74楼</a>
<a name="3930150" id="comment_anchor_3930150"></a>

 
<span class="comment_date">2018-03-22 13:57</span>

 

            <a id="a_comment_author_3930150" href="https://home.cnblogs.com/u/1221152/" target="_blank">Sunjw110</a>

			</div>
			<div class="feedbackCon">
				
<div id="comment_body_3930150" class="blog_comment_body">
    V那里应该是n*r吧，r*n根本没法乘
</div>
        <div class="comment_vote">
            <a href="javascript:void(0);" class="comment_digg" onclick="return voteComment(&#39;3930150&#39;, &#39;Digg&#39;, this);">
                支持(0)
            </a>
            <a href="javascript:void(0);" class="comment_burry" onclick="return voteComment(&#39;3930150&#39;, &#39;Bury&#39;, this);">
                反对(0)
            </a>
        </div>
        

			</div>
		</div>
		<div class="feedbackItem">
			<div class="feedbackListSubtitle">
				<div class="feedbackManage">
					&nbsp;&nbsp;

<span class="comment_actions">
    
    
    
    
</span>


				</div>
				
<a href="https://www.cnblogs.com/LeftNotEasy/archive/2011/01/19/svd-and-applications.html#3995416" class="layer">#75楼</a>
<a name="3995416" id="comment_anchor_3995416"></a>

 
<span class="comment_date">2018-06-11 09:34</span>

 

            <a id="a_comment_author_3995416" href="https://www.cnblogs.com/zzy0471/" target="_blank">会长</a>

			</div>
			<div class="feedbackCon">
				
<div id="comment_body_3995416" class="blog_comment_body">
    请问您什么书里有对奇异值分解的详细介绍，可否推荐，谢谢。我有一本《线性代数及其应用》里面并没有看到相关内容。
</div>
        <div class="comment_vote">
            <a href="javascript:void(0);" class="comment_digg" onclick="return voteComment(&#39;3995416&#39;, &#39;Digg&#39;, this);">
                支持(0)
            </a>
            <a href="javascript:void(0);" class="comment_burry" onclick="return voteComment(&#39;3995416&#39;, &#39;Bury&#39;, this);">
                反对(0)
            </a>
        </div>
        <span id="comment_3995416_avatar" style="display:none">
            https://pic.cnblogs.com/face/29548/20180105094720.png
        </span>

			</div>
		</div>
		<div class="feedbackItem">
			<div class="feedbackListSubtitle">
				<div class="feedbackManage">
					&nbsp;&nbsp;

<span class="comment_actions">
    
    
    
    
</span>


				</div>
				
<a href="https://www.cnblogs.com/LeftNotEasy/archive/2011/01/19/svd-and-applications.html#4034226" class="layer">#76楼</a>
<a name="4034226" id="comment_anchor_4034226"></a>

 
<span class="comment_date">2018-08-02 16:18</span>

 

            <a id="a_comment_author_4034226" href="https://home.cnblogs.com/u/1026587/" target="_blank">wujinjun</a>

			</div>
			<div class="feedbackCon">
				
<div id="comment_body_4034226" class="blog_comment_body">
    写的很好！赞一个！！！
</div>
        <div class="comment_vote">
            <a href="javascript:void(0);" class="comment_digg" onclick="return voteComment(&#39;4034226&#39;, &#39;Digg&#39;, this);">
                支持(0)
            </a>
            <a href="javascript:void(0);" class="comment_burry" onclick="return voteComment(&#39;4034226&#39;, &#39;Bury&#39;, this);">
                反对(0)
            </a>
        </div>
        

			</div>
		</div>
		<div class="feedbackItem">
			<div class="feedbackListSubtitle">
				<div class="feedbackManage">
					&nbsp;&nbsp;

<span class="comment_actions">
    
    
    
    
</span>


				</div>
				
<a href="https://www.cnblogs.com/LeftNotEasy/archive/2011/01/19/svd-and-applications.html#4106447" class="layer">#77楼</a>
<a name="4106447" id="comment_anchor_4106447"></a>

 
<span class="comment_date">2018-11-05 15:22</span>

 

            <a id="a_comment_author_4106447" href="https://www.cnblogs.com/zsl96/" target="_blank">浅踏雪无痕</a>

			</div>
			<div class="feedbackCon">
				
<div id="comment_body_4106447" class="blog_comment_body">
    多谢
</div>
        <div class="comment_vote">
            <a href="javascript:void(0);" class="comment_digg" onclick="return voteComment(&#39;4106447&#39;, &#39;Digg&#39;, this);">
                支持(0)
            </a>
            <a href="javascript:void(0);" class="comment_burry" onclick="return voteComment(&#39;4106447&#39;, &#39;Bury&#39;, this);">
                反对(0)
            </a>
        </div>
        <span id="comment_4106447_avatar" style="display:none">
            https://pic.cnblogs.com/face/1354767/20180318150103.png
        </span>

			</div>
		</div>
		<div class="feedbackItem">
			<div class="feedbackListSubtitle">
				<div class="feedbackManage">
					&nbsp;&nbsp;

<span class="comment_actions">
    
    
    
    
</span>


				</div>
				
<a href="https://www.cnblogs.com/LeftNotEasy/archive/2011/01/19/svd-and-applications.html#4108072" class="layer">#78楼</a>
<a name="4108072" id="comment_anchor_4108072"></a>

 
<span class="comment_date">2018-11-06 22:35</span>

 

            <a id="a_comment_author_4108072" href="https://home.cnblogs.com/u/1255782/" target="_blank">奇怪的乐乐</a>

			</div>
			<div class="feedbackCon">
				
<div id="comment_body_4108072" class="blog_comment_body">
    感谢博主，中文网站上，好多信息都是复制粘贴的，像博主这样愿意花时间精力，分享自己的见解的人真的太不容易了~<br>在cs231n中看到svd这个概念，有点懵，看到你的博文，真的就好多了，有一个完整的概念，为博主点赞~
</div>
        <div class="comment_vote">
            <a href="javascript:void(0);" class="comment_digg" onclick="return voteComment(&#39;4108072&#39;, &#39;Digg&#39;, this);">
                支持(0)
            </a>
            <a href="javascript:void(0);" class="comment_burry" onclick="return voteComment(&#39;4108072&#39;, &#39;Bury&#39;, this);">
                反对(0)
            </a>
        </div>
        

			</div>
		</div>
		<div class="feedbackItem">
			<div class="feedbackListSubtitle">
				<div class="feedbackManage">
					&nbsp;&nbsp;

<span class="comment_actions">
    
    
    
    
</span>


				</div>
				
<a href="https://www.cnblogs.com/LeftNotEasy/archive/2011/01/19/svd-and-applications.html#4109093" class="layer">#79楼</a>
<a name="4109093" id="comment_anchor_4109093"></a>

 
<span class="comment_date">2018-11-07 19:32</span>

 

            <a id="a_comment_author_4109093" href="https://home.cnblogs.com/u/1081669/" target="_blank">杨梅核</a>

			</div>
			<div class="feedbackCon">
				
<div id="comment_body_4109093" class="blog_comment_body">
    大大的一个赞
</div>
        <div class="comment_vote">
            <a href="javascript:void(0);" class="comment_digg" onclick="return voteComment(&#39;4109093&#39;, &#39;Digg&#39;, this);">
                支持(0)
            </a>
            <a href="javascript:void(0);" class="comment_burry" onclick="return voteComment(&#39;4109093&#39;, &#39;Bury&#39;, this);">
                反对(0)
            </a>
        </div>
        

			</div>
		</div>
		<div class="feedbackItem">
			<div class="feedbackListSubtitle">
				<div class="feedbackManage">
					&nbsp;&nbsp;

<span class="comment_actions">
    
    
    
    
</span>


				</div>
				
<a href="https://www.cnblogs.com/LeftNotEasy/archive/2011/01/19/svd-and-applications.html#4116950" class="layer">#80楼</a>
<a name="4116950" id="comment_anchor_4116950"></a>

 
<span class="comment_date">2018-11-18 14:27</span>

 

            <a id="a_comment_author_4116950" href="https://home.cnblogs.com/u/1538335/" target="_blank">LizAMER</a>

			</div>
			<div class="feedbackCon">
				
<div id="comment_body_4116950" class="blog_comment_body">
    <a href="https://www.cnblogs.com/LeftNotEasy/archive/2011/01/19/svd-and-applications.html#3563159" title="查看所回复的评论" onclick="commentManager.renderComments(0,50,3563159);">@</a>
Sophie冯 线性变换不特指2维的，重点是“线性”的。原文博主的说法并没有错，你只要看过任何一本线代书就知道。
</div>
        <div class="comment_vote">
            <a href="javascript:void(0);" class="comment_digg" onclick="return voteComment(&#39;4116950&#39;, &#39;Digg&#39;, this);">
                支持(0)
            </a>
            <a href="javascript:void(0);" class="comment_burry" onclick="return voteComment(&#39;4116950&#39;, &#39;Bury&#39;, this);">
                反对(0)
            </a>
        </div>
        

			</div>
		</div>
		<div class="feedbackItem">
			<div class="feedbackListSubtitle">
				<div class="feedbackManage">
					&nbsp;&nbsp;

<span class="comment_actions">
    
    
    
    
</span>


				</div>
				
<a href="https://www.cnblogs.com/LeftNotEasy/archive/2011/01/19/svd-and-applications.html#4137369" class="layer">#81楼</a>
<a name="4137369" id="comment_anchor_4137369"></a>

        <span id="comment-maxId" style="display:none">4137369</span>
        <span id="comment-maxDate" style="display:none">2018/12/12 上午10:49:54</span>
 
<span class="comment_date">2018-12-12 10:49</span>

 

            <a id="a_comment_author_4137369" href="https://www.cnblogs.com/kjkj/" target="_blank">巴拉巴拉程序猿</a>

			</div>
			<div class="feedbackCon">
				
<div id="comment_body_4137369" class="blog_comment_body">
    您好，请问既然对原矩阵的转置进行奇异值分解会使得左右奇异向量的意义产生变化，那为什么您第四部分对左右奇异向量的意义跟《数学之美》中的含义是一致的呢？不应该是反着的吗
</div>
        <div class="comment_vote">
            <a href="javascript:void(0);" class="comment_digg" onclick="return voteComment(&#39;4137369&#39;, &#39;Digg&#39;, this);">
                支持(0)
            </a>
            <a href="javascript:void(0);" class="comment_burry" onclick="return voteComment(&#39;4137369&#39;, &#39;Bury&#39;, this);">
                反对(0)
            </a>
        </div>
        <span id="comment_4137369_avatar" style="display:none">
            https://pic.cnblogs.com/face/853036/20181024111024.png
        </span>

			</div>
		</div>

<div id="comment_pager_bottom">
    
    <div class="pager">
        <!-- 上一页 -->
        <a href="https://www.cnblogs.com/LeftNotEasy/archive/2011/01/19/svd-and-applications.html#!comments" onclick="commentManager.renderComments(1, 50); return false;">
            &lt; Prev
        </a>

        <!-- 第一页 -->
        <a href="https://www.cnblogs.com/LeftNotEasy/archive/2011/01/19/svd-and-applications.html#!comments" onclick="commentManager.renderComments(1, 50); return false;">
            1
        </a>

        <!-- 前半部分页码 -->
        

        <!-- 当前页 -->
        <span class="current">2</span>

        <!-- 后半部分页码 -->

        
        <!-- 末尾页 -->
        

        <!-- 下一页 -->
        
    </div>
</div>


</div>
<script>
    var commentManager = new blogCommentManager();
    commentManager.renderComments(0);
</script>

<div id="comment_form" class="commentform">
    <a name="commentform"></a>
    <div id="divCommentShow"></div>
    <div id="comment_nav"><span id="span_refresh_tips"></span><a href="javascript:void(0);" onclick="return RefreshCommentList();" id="lnk_RefreshComments" runat="server" clientidmode="Static">刷新评论</a><a href="https://www.cnblogs.com/LeftNotEasy/archive/2011/01/19/svd-and-applications.html#" onclick="return RefreshPage();">刷新页面</a><a href="https://www.cnblogs.com/LeftNotEasy/archive/2011/01/19/svd-and-applications.html#top">返回顶部</a></div>
    <div id="comment_form_container"><div class="login_tips">
    注册用户登录后才能发表评论，请 
    <a rel="nofollow" href="javascript:void(0);" class="underline" onclick="return login(&#39;commentform&#39;);">登录</a>
     或 
    <a rel="nofollow" href="javascript:void(0);" class="underline" onclick="return register();">注册</a>，
    <a href="https://www.cnblogs.com/">访问</a> 网站首页。
</div></div>
    <div class="ad_text_commentbox" id="ad_text_under_commentbox"></div>
    <div id="ad_t2"><a href="http://www.ucancode.com/index.htm" target="_blank" onclick="ga(&#39;send&#39;, &#39;event&#39;, &#39;Link&#39;, &#39;click&#39;, &#39;T2-工控&#39;)">【推荐】超50万C++/C#源码: 大型实时仿真组态图形源码</a><br><a href="http://click.aliyun.com/m/1000074461/" target="_blank" onclick="ga(&#39;send&#39;, &#39;event&#39;, &#39;Link&#39;, &#39;click&#39;, &#39;T2-阿里云&#39;)">【活动】阿里云910会员节多款云产品满减活动火热进行中</a><br><a href="https://www.ctyun.cn/activity/#/enterprise2?hmsr=%E5%8D%9A%E5%AE%A2%E5%9B%AD-0901-0%E5%85%83%E4%B8%8A%E4%BA%91&amp;hmpl=&amp;hmcu=&amp;hmkw=&amp;hmci=" target="_blank" onclick="ga(&#39;send&#39;, &#39;event&#39;, &#39;Link&#39;, &#39;click&#39;, &#39;T2-天翼云&#39;)">【推荐】新手上天翼云，数十款云产品、新一代主机0元体验</a><br><a href="http://clickc.admaster.com.cn/c/a131574,b3595115,c1705,i0,m101,8a1,8b3,h" target="_blank" onclick="ga(&#39;send&#39;, &#39;event&#39;, &#39;Link&#39;, &#39;click&#39;, &#39;T2-华为云微认证&#39;)">【推荐】零基础轻松玩转华为云产品，获壕礼加返百元大礼</a><br><a href="http://clickc.admaster.com.cn/c/a131575,b3595121,c1705,i0,m101,8a1,8b3,h" target="_blank" onclick="ga(&#39;send&#39;, &#39;event&#39;, &#39;Link&#39;, &#39;click&#39;, &#39;T2-华为文字&#39;)">【推荐】华为云文字识别资源包重磅上市，1元万次限时抢购</a><br></div>
    <div id="opt_under_post"></div>
    <script async="async" src="./机器学习中的数学(5)-强大的矩阵奇异值分解(SVD)及其应用 - LeftNotEasy - 博客园_files/gpt.js"></script>
    <script>
        var googletag = googletag || {};
        googletag.cmd = googletag.cmd || [];
    </script>
    <script>
        googletag.cmd.push(function () {
            googletag.defineSlot("/1090369/C1", [300, 250], "div-gpt-ad-1546353474406-0").addService(googletag.pubads());
            googletag.defineSlot("/1090369/C2", [468, 60], "div-gpt-ad-1539008685004-0").addService(googletag.pubads());
            googletag.pubads().enableSingleRequest();
            googletag.enableServices();
        });
    </script>
    <div id="cnblogs_c1" class="c_ad_block">
        <div id="div-gpt-ad-1546353474406-0" style="height:250px; width:300px;" data-google-query-id="CNS9yInhx-QCFUcklgodk60Eiw"><div id="google_ads_iframe_/1090369/C1_0__container__" style="border: 0pt none; display: inline-block; width: 300px; height: 250px;"><iframe frameborder="0" src="./机器学习中的数学(5)-强大的矩阵奇异值分解(SVD)及其应用 - LeftNotEasy - 博客园_files/container.html" id="google_ads_iframe_/1090369/C1_0" title="3rd party ad content" name="" scrolling="no" marginwidth="0" marginheight="0" width="300" height="250" data-is-safeframe="true" sandbox="allow-forms allow-pointer-lock allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-top-navigation-by-user-activation" data-google-container-id="1" style="border: 0px; vertical-align: bottom;" data-load-complete="true"></iframe></div></div>
    </div>
    <div id="under_post_news"><div class="recomm-block"><b>相关博文：</b><br>·  <a title="强大的矩阵奇异值分解(SVD)" href="https://www.cnblogs.com/nolonely/p/7308496.html" target="_blank" onclick="clickRecomItmem(7308496)">强大的矩阵奇异值分解(SVD)</a><br>·  <a title="机器学习中的数学(5)-强大的矩阵奇异值分解(SVD)及其应用" href="https://www.cnblogs.com/donaldlee2008/p/5237100.html" target="_blank" onclick="clickRecomItmem(5237100)">机器学习中的数学(5)-强大的矩阵奇异值分解(SVD)及其应用</a><br>·  <a title="机器学习中的数学(5)-强大的矩阵奇异值分解(SVD)及其应用" href="https://www.cnblogs.com/nsnow/p/4771888.html" target="_blank" onclick="clickRecomItmem(4771888)">机器学习中的数学(5)-强大的矩阵奇异值分解(SVD)及其应用</a><br>·  <a title="矩阵奇异值分解(SVD)及其应用" href="https://www.cnblogs.com/ixiaoge/p/3851963.html" target="_blank" onclick="clickRecomItmem(3851963)">矩阵奇异值分解(SVD)及其应用</a><br>·  <a title="机器学习中的数学(5)-强大的矩阵奇异值分解(SVD)及其应用" href="https://www.cnblogs.com/huzs/p/3741975.html" target="_blank" onclick="clickRecomItmem(3741975)">机器学习中的数学(5)-强大的矩阵奇异值分解(SVD)及其应用</a><br></div></div>
    <div id="cnblogs_c2" class="c_ad_block">
        <div id="div-gpt-ad-1539008685004-0" style="height:60px; width:468px;" data-google-query-id="CL6bx4nhx-QCFUcklgodk60Eiw">
            
        <div id="google_ads_iframe_/1090369/C2_0__container__" style="border: 0pt none;"><iframe id="google_ads_iframe_/1090369/C2_0" title="3rd party ad content" name="google_ads_iframe_/1090369/C2_0" width="468" height="60" scrolling="no" marginwidth="0" marginheight="0" frameborder="0" style="border: 0px; vertical-align: bottom;" data-google-container-id="2" data-load-complete="true" src="./机器学习中的数学(5)-强大的矩阵奇异值分解(SVD)及其应用 - LeftNotEasy - 博客园_files/saved_resource.html"></iframe></div></div>
    </div>
    <div id="under_post_kb">
<div class="itnews c_ad_block">
    <b>最新 IT 新闻</b>:
    <br>
 ·              <a href="https://news.cnblogs.com/n/636867/" target="_blank">非洲猪瘟防控技术标准规范发布 疫苗临床试验在即</a>
            <br>
 ·              <a href="https://news.cnblogs.com/n/636866/" target="_blank">谷歌正在 Chrome 78 中实验 DoH</a>
            <br>
 ·              <a href="https://news.cnblogs.com/n/636865/" target="_blank">云时代编程语言 Ballerina 发布：轻松创建跨分布式端的弹性服务</a>
            <br>
 ·              <a href="https://news.cnblogs.com/n/636864/" target="_blank">现场视频来了！摇滚Jack Ma年会朋克风唱《怒放的生命》</a>
            <br>
 ·              <a href="https://news.cnblogs.com/n/636863/" target="_blank">iPhone 11发布 京东成中国区唯一官方授权预售渠道</a>
            <br>
    » <a href="https://news.cnblogs.com/" title="IT 新闻" target="_blank">更多新闻...</a>
</div></div>
    <div id="HistoryToday" class="c_ad_block"></div>
    <script type="text/javascript">
        fixPostBody();
        setTimeout(function () { incrementViewCount(cb_entryId); }, 50);
        deliverAdT2();
        deliverAdC1();
        deliverAdC2();
        loadNewsAndKb();
        loadBlogSignature();
LoadPostCategoriesTags(cb_blogId, cb_entryId);        LoadPostInfoBlock(cb_blogId, cb_entryId, cb_blogApp, cb_blogUserGuid);
        GetPrevNextPost(cb_entryId, cb_blogId, cb_entryCreatedDate, cb_postType);
        loadOptUnderPost();
        GetHistoryToday(cb_blogId, cb_blogApp, cb_entryCreatedDate);
    </script>
</div>
	</div><!--end: forFlow -->
	</div><!--end: mainContent 主体内容容器-->

	<div id="sideBar">
		<div id="sideBarMain">
			


			<div id="blog-calendar" style="display:none"></div><script>loadBlogDefaultCalendar();</script>
			
			<div id="leftcontentcontainer">
				<div id="blog-sidecolumn">

<!-- 搜索 -->
<div id="sidebar_search" class="sidebar-block">
    
</div>

<!-- 常用链接 -->
<div id="sidebar_shortcut" class="sidebar-block">
    <div class="catListLink">
<h3 class="catListTitle">
常用链接
</h3>
<ul>
		<li>

<a href="https://www.cnblogs.com/LeftNotEasy/p/" title="我的博客的随笔列表">我的随笔</a>
</li>
		<li>

<a href="https://www.cnblogs.com/LeftNotEasy/MyComments.html" title="我的发表过的评论列表">我的评论</a>
</li>
		<li>

<a href="https://www.cnblogs.com/LeftNotEasy/OtherPosts.html" title="我评论过的随笔列表">我的参与</a>
</li>
		<li>

<a href="https://www.cnblogs.com/LeftNotEasy/RecentComments.html" title="我的博客的评论列表">最新评论</a>
</li>
		<li>

<a href="https://www.cnblogs.com/LeftNotEasy/tag/" title="我的博客的标签列表">我的标签</a>
</li>

</ul>
<div id="itemListLin_con" style="display:none;">
<ul>

</ul>
</div>
</div>


</div>

<!-- 最新随笔 -->



<!-- 我的标签 -->
<div id="sidebar_toptags" class="sidebar-block">
    <div class="catListTag">
<h3 class="catListTitle">我的标签</h3>
<ul>

        <li>
            <a href="https://www.cnblogs.com/LeftNotEasy/tag/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a>(8)
        </li>
        <li>
            <a href="https://www.cnblogs.com/LeftNotEasy/tag/%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E/">搜索引擎</a>(7)
        </li>
        <li>
            <a href="https://www.cnblogs.com/LeftNotEasy/tag/Lucene/">Lucene</a>(6)
        </li>
        <li>
            <a href="https://www.cnblogs.com/LeftNotEasy/tag/machine%20learning/">machine learning</a>(4)
        </li>
        <li>
            <a href="https://www.cnblogs.com/LeftNotEasy/tag/Hadoop/">Hadoop</a>(4)
        </li>
        <li>
            <a href="https://www.cnblogs.com/LeftNotEasy/tag/mathmatics/">mathmatics</a>(3)
        </li>
        <li>
            <a href="https://www.cnblogs.com/LeftNotEasy/tag/pymining/">pymining</a>(3)
        </li>
        <li>
            <a href="https://www.cnblogs.com/LeftNotEasy/tag/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/">人工智能</a>(3)
        </li>
        <li>
            <a href="https://www.cnblogs.com/LeftNotEasy/tag/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98/">数据挖掘</a>(3)
        </li>
        <li>
            <a href="https://www.cnblogs.com/LeftNotEasy/tag/PCA/">PCA</a>(2)
        </li>
            <li>
                <a href="https://www.cnblogs.com/LeftNotEasy/tag/">更多</a>
            </li>

</ul>
</div>


</div>

<!-- 积分与排名 -->


<!-- 随笔分类、随笔档案、文章分类、新闻分类、相册、链接 -->
<div id="sidebar_categories">
    
        <div id="sidebar_postcategory" class="catListPostCategory sidebar-block">
            <h3 class="catListTitle">
                

随笔分类



            </h3>


            <ul>

                        <li>
                            
<a href="https://www.cnblogs.com/LeftNotEasy/category/355677.html" rel="" target="">
    Hadoop(4)
</a>
 

                        </li>
                        <li>
                            
<a href="https://www.cnblogs.com/LeftNotEasy/category/220705.html" rel="" target="">
    Lucene C++重写心得(2)
</a>
 

                        </li>
                        <li>
                            
<a href="https://www.cnblogs.com/LeftNotEasy/category/220704.html" rel="" target="">
    Lucene JAVA心得(9)
</a>
 

                        </li>
                        <li>
                            
<a href="https://www.cnblogs.com/LeftNotEasy/category/220702.html" rel="" target="">
    安排，计划，总结(3)
</a>
 

                        </li>
                        <li>
                            
<a href="https://www.cnblogs.com/LeftNotEasy/category/220709.html" rel="" target="">
    分布式存储(2)
</a>
 

                        </li>
                        <li>
                            
<a href="https://www.cnblogs.com/LeftNotEasy/category/273622.html" rel="" target="">
    机器学习(11)
</a>
 

                        </li>
                        <li>
                            
<a href="https://www.cnblogs.com/LeftNotEasy/category/261428.html" rel="" target="">
    结构设计(1)
</a>
 

                        </li>
                        <li>
                            
<a href="https://www.cnblogs.com/LeftNotEasy/category/273623.html" rel="" target="">
    数学(8)
</a>
 

                        </li>

            </ul>


        </div>
        <div id="sidebar_postarchive" class="catListPostArchive sidebar-block">
            <h3 class="catListTitle">
                

随笔档案



            </h3>


            <ul>

                        <li>
                            
<a href="https://www.cnblogs.com/LeftNotEasy/archive/2017/11.html" rel="" target="">
    2017年11月(1)
</a>
 

                        </li>
                        <li>
                            
<a href="https://www.cnblogs.com/LeftNotEasy/archive/2016/12.html" rel="" target="">
    2016年12月(2)
</a>
 

                        </li>
                        <li>
                            
<a href="https://www.cnblogs.com/LeftNotEasy/archive/2016/11.html" rel="" target="">
    2016年11月(1)
</a>
 

                        </li>
                        <li>
                            
<a href="https://www.cnblogs.com/LeftNotEasy/archive/2016/08.html" rel="" target="">
    2016年8月(2)
</a>
 

                        </li>
                        <li>
                            
<a href="https://www.cnblogs.com/LeftNotEasy/archive/2016/07.html" rel="" target="">
    2016年7月(2)
</a>
 

                        </li>
                        <li>
                            
<a href="https://www.cnblogs.com/LeftNotEasy/archive/2012/02.html" rel="" target="">
    2012年2月(1)
</a>
 

                        </li>
                        <li>
                            
<a href="https://www.cnblogs.com/LeftNotEasy/archive/2011/08.html" rel="" target="">
    2011年8月(1)
</a>
 

                        </li>
                        <li>
                            
<a href="https://www.cnblogs.com/LeftNotEasy/archive/2011/05.html" rel="" target="">
    2011年5月(5)
</a>
 

                        </li>
                        <li>
                            
<a href="https://www.cnblogs.com/LeftNotEasy/archive/2011/03.html" rel="" target="">
    2011年3月(1)
</a>
 

                        </li>
                        <li>
                            
<a href="https://www.cnblogs.com/LeftNotEasy/archive/2011/02.html" rel="" target="">
    2011年2月(1)
</a>
 

                        </li>
                        <li>
                            
<a href="https://www.cnblogs.com/LeftNotEasy/archive/2011/01.html" rel="" target="">
    2011年1月(3)
</a>
 

                        </li>
                        <li>
                            
<a href="https://www.cnblogs.com/LeftNotEasy/archive/2010/12.html" rel="" target="">
    2010年12月(2)
</a>
 

                        </li>
                        <li>
                            
<a href="https://www.cnblogs.com/LeftNotEasy/archive/2010/11.html" rel="" target="">
    2010年11月(1)
</a>
 

                        </li>
                        <li>
                            
<a href="https://www.cnblogs.com/LeftNotEasy/archive/2010/10.html" rel="" target="">
    2010年10月(1)
</a>
 

                        </li>
                        <li>
                            
<a href="https://www.cnblogs.com/LeftNotEasy/archive/2010/09.html" rel="" target="">
    2010年9月(2)
</a>
 

                        </li>
                        <li>
                            
<a href="https://www.cnblogs.com/LeftNotEasy/archive/2010/08.html" rel="" target="">
    2010年8月(1)
</a>
 

                        </li>
                        <li>
                            
<a href="https://www.cnblogs.com/LeftNotEasy/archive/2010/01.html" rel="" target="">
    2010年1月(12)
</a>
 

                        </li>
                        <li>
                            
<a href="https://www.cnblogs.com/LeftNotEasy/archive/2009/12.html" rel="" target="">
    2009年12月(2)
</a>
 

                        </li>
                        <li>
                            
<a href="https://www.cnblogs.com/LeftNotEasy/archive/2009/11.html" rel="" target="">
    2009年11月(4)
</a>
 

                        </li>

            </ul>


        </div>

</div>

<!-- 最新评论 -->
<div id="sidebar_recentcomments" class="sidebar-block">
    <div class="catListComment">
<h3 class="catListTitle">最新评论</h3>

	<div class="RecentCommentBlock">
        <ul>
                    <li class="recent_comment_title"><a href="https://www.cnblogs.com/LeftNotEasy/archive/2011/05/02/basic-of-svm.html#4342628">1. Re:机器学习中的算法(2)-支持向量机(SVM)基础</a></li>
                    <li class="recent_comment_body">赞</li>
                    <li class="recent_comment_author">--乘着风去破浪</li>
                    <li class="recent_comment_title"><a href="https://www.cnblogs.com/LeftNotEasy/p/mle-cross-entropy-and-deep-learning.html#4335255">2. Re:最大似然估计 (Maximum Likelihood Estimation), 交叉熵 (Cross Entropy) 与深度神经网络</a></li>
                    <li class="recent_comment_body">博主写的不错呀，怎么不更新了呐</li>
                    <li class="recent_comment_author">--Jx_ForeverYoung</li>
                    <li class="recent_comment_title"><a href="https://www.cnblogs.com/LeftNotEasy/archive/2011/05/02/basic-of-svm.html#4229607">3. Re:机器学习中的算法(2)-支持向量机(SVM)基础</a></li>
                    <li class="recent_comment_body">楼主，能否将核函数用个通俗的类比法说明下？SVM基本原理是看懂了，就是在这个核函数的概念上，一直不太理解。。我从李航老师的统计学习方法书中，看到了这样的描述：核技巧的想法是，在学习与预测中只定义核函数...</li>
                    <li class="recent_comment_author">--老笨啊</li>
                    <li class="recent_comment_title"><a href="https://www.cnblogs.com/LeftNotEasy/archive/2011/01/08/lda-and-pca-machine-learning.html#4140047">4. Re:机器学习中的数学(4)-线性判别分析（LDA）, 主成分分析(PCA)</a></li>
                    <li class="recent_comment_body">知识不过时，现在看来依然获益匪浅，感谢！</li>
                    <li class="recent_comment_author">--BenChur</li>
                    <li class="recent_comment_title"><a href="https://www.cnblogs.com/LeftNotEasy/archive/2011/01/19/svd-and-applications.html#4137369">5. Re:机器学习中的数学(5)-强大的矩阵奇异值分解(SVD)及其应用</a></li>
                    <li class="recent_comment_body">您好，请问既然对原矩阵的转置进行奇异值分解会使得左右奇异向量的意义产生变化，那为什么您第四部分对左右奇异向量的意义跟《数学之美》中的含义是一致的呢？不应该是反着的吗</li>
                    <li class="recent_comment_author">--巴拉巴拉程序猿</li>
        </ul>
    </div>
</div>


</div>



<!-- 阅读排行榜 -->
<div id="sidebar_topviewedposts" class="sidebar-block">
    <div class="catListView">
<h3 class="catListTitle">阅读排行榜</h3>
	<div id="TopViewPostsBlock">
        <ul style="word-break:break-all">
                    <li>
                        <a href="https://www.cnblogs.com/LeftNotEasy/archive/2011/01/19/svd-and-applications.html">
                            1. 机器学习中的数学(5)-强大的矩阵奇异值分解(SVD)及其应用(351819)
                        </a>
                    </li>
                    <li>
                        <a href="https://www.cnblogs.com/LeftNotEasy/archive/2011/03/07/random-forest-and-gbdt.html">
                            2. 机器学习中的算法(1)-决策树模型组合之随机森林与GBDT(213156)
                        </a>
                    </li>
                    <li>
                        <a href="https://www.cnblogs.com/LeftNotEasy/archive/2011/01/08/lda-and-pca-machine-learning.html">
                            3. 机器学习中的数学(4)-线性判别分析（LDA）, 主成分分析(PCA)(168982)
                        </a>
                    </li>
                    <li>
                        <a href="https://www.cnblogs.com/LeftNotEasy/archive/2011/05/02/basic-of-svm.html">
                            4. 机器学习中的算法(2)-支持向量机(SVM)基础(162325)
                        </a>
                    </li>
                    <li>
                        <a href="https://www.cnblogs.com/LeftNotEasy/archive/2010/12/05/mathmatic_in_machine_learning_1_regression_and_gradient_descent.html">
                            5. 机器学习中的数学(1)-回归(regression)、梯度下降(gradient descent)(134651)
                        </a>
                    </li>
        </ul>
    </div>
</div>


</div>

<!-- 评论排行榜 -->
<div id="sidebar_topcommentedposts" class="sidebar-block">
    <div class="catListFeedback">
<h3 class="catListTitle">评论排行榜</h3>
	<div id="TopFeedbackPostsBlock">
        <ul style="word-break:break-all">
                    <li>
                        <a href="https://www.cnblogs.com/LeftNotEasy/archive/2011/01/19/svd-and-applications.html">
                            1. 机器学习中的数学(5)-强大的矩阵奇异值分解(SVD)及其应用(81)
                        </a>
                    </li>
                    <li>
                        <a href="https://www.cnblogs.com/LeftNotEasy/archive/2010/12/05/mathmatic_in_machine_learning_1_regression_and_gradient_descent.html">
                            2. 机器学习中的数学(1)-回归(regression)、梯度下降(gradient descent)(42)
                        </a>
                    </li>
                    <li>
                        <a href="https://www.cnblogs.com/LeftNotEasy/archive/2011/05/02/basic-of-svm.html">
                            3. 机器学习中的算法(2)-支持向量机(SVM)基础(42)
                        </a>
                    </li>
                    <li>
                        <a href="https://www.cnblogs.com/LeftNotEasy/archive/2011/01/08/lda-and-pca-machine-learning.html">
                            4. 机器学习中的数学(4)-线性判别分析（LDA）, 主成分分析(PCA)(38)
                        </a>
                    </li>
                    <li>
                        <a href="https://www.cnblogs.com/LeftNotEasy/p/choice-of-programmer.html">
                            5. 程序员的选择(36)
                        </a>
                    </li>
        </ul>
    </div>
</div>


</div>

<!-- 推荐排行榜 -->
<div id="sidebar_topdiggedposts" class="sidebar-block">
    
<div id="topdigg_posts_wrap">
    <div class="catListView">
        <h3 class="catListTitle">推荐排行榜</h3>
        <div id="TopDiggPostsBlock">
            <ul style="word-break: break-all">
                        <li>
                            <a href="https://www.cnblogs.com/LeftNotEasy/archive/2011/01/19/svd-and-applications.html">
                                1. 机器学习中的数学(5)-强大的矩阵奇异值分解(SVD)及其应用(119)
                            </a>
                        </li>
                        <li>
                            <a href="https://www.cnblogs.com/LeftNotEasy/p/choice-of-programmer.html">
                                2. 程序员的选择(63)
                            </a>
                        </li>
                        <li>
                            <a href="https://www.cnblogs.com/LeftNotEasy/archive/2011/05/02/basic-of-svm.html">
                                3. 机器学习中的算法(2)-支持向量机(SVM)基础(47)
                            </a>
                        </li>
                        <li>
                            <a href="https://www.cnblogs.com/LeftNotEasy/archive/2010/12/05/mathmatic_in_machine_learning_1_regression_and_gradient_descent.html">
                                4. 机器学习中的数学(1)-回归(regression)、梯度下降(gradient descent)(38)
                            </a>
                        </li>
                        <li>
                            <a href="https://www.cnblogs.com/LeftNotEasy/archive/2011/03/07/random-forest-and-gbdt.html">
                                5. 机器学习中的算法(1)-决策树模型组合之随机森林与GBDT(34)
                            </a>
                        </li>
            </ul>
        </div>
    </div>
</div>
</div></div>
                    <script>loadBlogSideColumn();</script>
			</div>
			
		</div><!--end: sideBarMain -->
	</div><!--end: sideBar 侧边栏容器 -->
	<div class="clear"></div>
	</div><!--end: main -->
	<div class="clear"></div>
	<div id="footer">
		<!--done-->
Copyright © 2019 LeftNotEasy
<br><span id="poweredby">Powered by .NET Core 3.0.0-preview9-19423-09 on Linux</span>



	</div><!--end: footer -->
</div><!--end: home 自定义的最大容器 -->


    


<iframe id="google_osd_static_frame_7032845677722" name="google_osd_static_frame" style="display: none; width: 0px; height: 0px;" src="./机器学习中的数学(5)-强大的矩阵奇异值分解(SVD)及其应用 - LeftNotEasy - 博客园_files/saved_resource(1).html"></iframe></body></html>